{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f255c8f5",
   "metadata": {},
   "source": [
    "[*prompt-llm*](https://github.com/SAP-samples/generative-ai-codejam/blob/main/exercises/03-prompt-llm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df15aaa",
   "metadata": {},
   "source": [
    "\n",
    "### Check your connection to Generative AI Hub\n",
    "\n",
    "‚òùÔ∏è In the init_env.py file the values from generative-ai-codejam/.aicore-config.json are assigned to environmental variables. That way the Generative AI Hub Python SDK will connect to Generative AI Hub.\n",
    "\n",
    "üëâ For the Python SDK to know which resource group to use, you also need to set the resource group in the variables.py file to your own resource group (e.g. team-01) that you created in the SAP AI Launchpad in exercise 00-connect-AICore-and-AILaunchpad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d001999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource group is set to: default\n"
     ]
    }
   ],
   "source": [
    "from config import init_env\n",
    "from config import variables\n",
    "import importlib\n",
    "variables = importlib.reload(variables)\n",
    "\n",
    "# TODO: You need to specify which model you want to use. In this case we are directing our prompt\n",
    "# to the openAI API directly so you need to pick one of the GPT models. Make sure the model is actually deployed\n",
    "# in genAI Hub. You might also want to chose a model that can also process images here already. \n",
    "# E.g. 'gpt-4.1-mini'\n",
    "MODEL_NAME = 'gpt-4o'\n",
    "\n",
    "# Do not modify the `assert` line below\n",
    "assert MODEL_NAME!='', \"\"\"You should change the variable `MODEL_NAME` with the name of your deployed model (like 'gpt-4o-mini') first!\"\"\"\n",
    "\n",
    "init_env.set_environment_variables()\n",
    "# Do not modify the `assert` line below \n",
    "assert variables.RESOURCE_GROUP!='', \"\"\"You should change the value assigned to the `RESOURCE_GROUP` in the `variables.py` file to your own resource group first!\"\"\"\n",
    "\n",
    "print(f\"Resource group is set to: {variables.RESOURCE_GROUP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d03d37c",
   "metadata": {},
   "source": [
    "\n",
    "### Prompt an LLM in the Generative AI Hub...\n",
    "\n",
    "...using OpenAI native client integration: https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/gen_ai_hub.html#openai\n",
    "\n",
    "To understand the API and structures, check OpenAI documentation: https://platform.openai.com/docs/guides/text?api-mode=chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d76f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The 2023 Nobel Prize in Chemistry was awarded to Moungi G. Bawendi, Louis E. Brus, and Alexei I. Ekimov for their discovery and synthesis of quantum dots. Quantum dots are tiny semiconductor particles or nanocrystals with unique optical and electronic properties due to their size, which is at the nanometer scale. These properties have significant applications in fields such as medical imaging and display technologies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "from IPython.display import Markdown\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Who won the latest Nobel prize of chemistry and what is their achievement?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "\n",
    "response = chat.completions.create(**kwargs)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3490b053",
   "metadata": {},
   "source": [
    "\n",
    "### Understanding roles\n",
    "\n",
    "Most LLMs have the roles system, assistant (GPT) or model (Gemini) and user that can be used to steer the models response. In the previous step you only used the role user to ask your question.\n",
    "\n",
    "üëâ Try out different system messages to change the response. You can also tell the model to not engage in smalltalk or only answer questions on a certain topic. Then try different user prompts as well!\n",
    "\n",
    "Please note, that in OpenAI API with o1 models and newer, developer messages replace the previous system messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12ca3077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Transformer architecture, it is. Layers of attention mechanisms, and feedforward networks, many have. Generate language, they do."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    {   \"role\": \"system\", \n",
    "        # TODO try changing the system prompt\n",
    "        \"content\": \"Speak like Yoda from Star Wars.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the underlying model architecture of an LLM? Explain it as short as possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e597c",
   "metadata": {},
   "source": [
    "üëâ Also try to have it speak like a pirate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42af0e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Arrr, the heart of an LLM be the transformer architecture, matey! It uses attention mechanisms to capture contextual relationships in the input text, allowing it to generate coherent and context-sensitive responses. Yo ho ho!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    {   \"role\": \"system\", \n",
    "        # TODO try changing the system prompt\n",
    "        \"content\": \"Speak like a pirate.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the underlying model architecture of an LLM? Explain it as short as possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fbea7",
   "metadata": {},
   "source": [
    "üëâ Also try to have it speak other celebraties who speaks in a way you are familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e5395b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Listen, folks, let me tell you‚Äîlarge language models, they‚Äôre incredible. The architecture, it‚Äôs known as a transformer. Highly powerful, revolutionary, uses attention mechanisms to understand context. Great technology, amazing performance, many say the best. That's what they are, folks!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    {   \"role\": \"system\", \n",
    "        # TODO try changing the system prompt\n",
    "        \"content\": \"Speak like a Donald Trump.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the underlying model architecture of an LLM? Explain it as short as possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79b929bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ÂêæÈóªÂ§ßÊ®°Âûã‰πãÂü∫Ôºå‰πÉÊòØÊ∑±Â±ÇÁ•ûÁªèÁΩëÁªú„ÄÇÂÖ∂Ë¶ÅÂ∑ßÂú®‰∫éËΩ¨ÂåñÂô®ÔºåËóèÊúâÂ§öÂ§¥Ëá™Ê≥®ÊÑèÊú∫Âà∂ÔºåÂ±ÇÂè†ËÄåÊàêÔºåÈöêÂê´ÂçÉ‰∏áÂèÇÊï∞ÔºåÊïÖËÉΩÈÄöÊôì‰∏áË±°Ôºö‰πâÁêÜÊ¥ûÊòéÔºåÊñáÈááÊñêÁÑ∂Ôºå‰πÉ‰∏ñÈó¥Êô∫ÊÖß‰πãÊòæÁé∞‰πü„ÄÇ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    {   \"role\": \"system\", \n",
    "        # TODO try changing the system prompt\n",
    "        \"content\": \"Speak like ÊùéÁôΩ in Chinese.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the underlying model architecture of an LLM? Explain it as short as possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2818785f",
   "metadata": {},
   "source": [
    "### Use Multimodal Models\n",
    "\n",
    "Multimodal models can use different inputs such as text, audio and images. In Generative AI Hub on SAP AI Core you can access multiple multimodal models (e.g. gpt-4o-mini).\n",
    "\n",
    "üëâ If you have not deployed one of the gpt-4o-mini models in previous exercises, then go back to the model library and deploy a model that can also process images.\n",
    "\n",
    "üëâ Now run the code snippet below to get a description for the image of the AI Foundation Architecture. These descriptions can then for example be used as alternative text for screen readers or other assistive tech.\n",
    "\n",
    "üëâ You can upload your own image and play around with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0335882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The image is a diagram labeled \"Single-tenant application (Retrieval Augmented Generation & Generative AI on SAP BTP).\" It illustrates a reference architecture within the SAP Business Technology Platform (BTP), showcasing how various components interact to leverage Generative AI.\n",
       "\n",
       "On the left, there's an icon representing a user connected via \"Application Clients\" like mobile or desktop. These clients connect to an \"App Router\" and \"HTML5 App Repository,\" which is part of a \"Subaccount\" in a Multi-Cloud environment.\n",
       "\n",
       "Within this subaccount, a \"User Interface\" section includes \"SAPUI5\" and \"UI5 Web Components.\" It also has \"SAP Continuous Integration and Delivery\" and \"SAP Business Application Studio.\"\n",
       "\n",
       "The diagram shows a pathway from the App Router to the \"SAP Authorization and Trust Management service,\" indicating trust relationships. Linked to this is the \"SAP Cloud Application Programming Model (CAP)\" section, which lists \"Application Service\" details like \"Use Case Logic,\" \"Data Management,\" \"LLM Plugins & SDKs,\" and \"SAP Cloud SDK for AI.\"\n",
       "\n",
       "A \"Destination\" line goes from the App Router to SAP's \"Generative AI Hub.\" This hub comprises \"SAP AI Launchpad\" and \"SAP AI Core,\" including elements like \"Trust & Control,\" \"Prompt Registry,\" and an \"Orchestration\" section with \"Grounding,\" \"Templating,\" \"Data Masking,\" and \"I/O Filtering.\"\n",
       "\n",
       "\"Foundation Model Access\" outlines \"Partner built\" and \"SAP built\" models, with an additional \"Foundation Models\" section for \"SAP hosted\" and \"Partner hosted.\" These connect to other areas via HTTPS.\n",
       "\n",
       "On the far right, there's a \"Network\" section connected by HTTPS, illustrating connections to \"SAP On-Premise Solutions,\" \"3rd Party Applications,\" and \"SAP Cloud Solutions\" alongside \"SAP Connectivity Service\" and \"SAP Destination Service.\"\n",
       "\n",
       "At the bottom, a legend defines symbols like \"Access,\" \"Mutual Trust,\" \"planned elements,\" and \"SAP BTP Service,\" with a note clarifying the diagram as an L2 level depiction. This diagram details how SAP's solutions utilize AI and integration across cloud and on-premise environments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "# get the image from the documents folder\n",
    "with open(\"images/ai-foundation-architecture.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe the images as an alternative text.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\", \n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/png;base64,{image_data}\"\n",
    "                        }\n",
    "                }\n",
    "            ]\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b75bf1c",
   "metadata": {},
   "source": [
    "\n",
    "### Extracting text from images\n",
    "\n",
    "Nora loves bananabread and thinks recipes are a good example of how LLMs can also extract complex text from images, like from a picture of a recipe of a bananabread. Try your own recipe if you like :)\n",
    "\n",
    "This exercise also shows how you can use the output of an LLM in other systems, as you can tell the LLM how to output information, for example in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f04aba31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the two JSON files based on the provided banana bread recipe image:\n",
       "\n",
       "**ingredients.json:**\n",
       "```json\n",
       "{\n",
       "  \"dry_ingredients\": {\n",
       "    \"all_purpose_flour\": \"260 g\",\n",
       "    \"sugar\": \"200 g\",\n",
       "    \"baking_soda\": \"6 g\",\n",
       "    \"salt\": \"3 g\"\n",
       "  },\n",
       "  \"wet_ingredients\": {\n",
       "    \"banana\": \"225 g\",\n",
       "    \"eggs\": \"2 large\",\n",
       "    \"vegetable_oil\": \"100 g\",\n",
       "    \"whole_milk\": \"55 g\",\n",
       "    \"vanilla_extract\": \"5 g\"\n",
       "  },\n",
       "  \"toppings\": {\n",
       "    \"chocolate_chips\": \"100 g\",\n",
       "    \"walnuts\": \"100 g (optional)\"\n",
       "  }\n",
       "}\n",
       "```\n",
       "\n",
       "**instructions.json:**\n",
       "```json\n",
       "{\n",
       "  \"directions\": [\n",
       "    \"Preheat oven to 180¬∞C.\",\n",
       "    \"Mash banana in a bowl.\",\n",
       "    \"Combine banana and other wet ingredients together in the same bowl.\",\n",
       "    \"Mix all dry ingredients together in a separate bowl.\",\n",
       "    \"Use a whisk to combine dry mixture into wet mixture until smooth.\",\n",
       "    \"Pour mixture into a greased/buttered loaf pan.\",\n",
       "    \"Place the pan into preheated oven on the middle rack and bake for about 60 minutes or until a toothpick comes out clean.\",\n",
       "    \"Enjoy!\"\n",
       "  ]\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "# get the image from the documents folder\n",
    "with open(\"images/bananabread.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Extract the ingredients and instructions in two different json files.\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "            }]\n",
    "        }]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
