{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0288dc",
   "metadata": {},
   "source": [
    "[*orchestration-service*](https://github.com/SAP-samples/generative-ai-codejam/blob/main/exercises/08-orchestration-service.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0884d",
   "metadata": {},
   "source": [
    "### Use the orchestration service of Generative AI Hub\n",
    "\n",
    "The orchestration service of Generative AI Hub lets you use all the available models with the same codebase. You only deploy the orchestration service and then you can access all available models simply by changing the model name parameter. You can also use grounding, prompt templating, data masking and content filtering capabilities.\n",
    "\n",
    "This code is based on the [AI180 TechEd 2024 Jump-Start session](https://github.com/SAP-samples/teched2024-AI180/tree/e648921c46337b57f61ecc9a93251d4b838d7ad0/exercises/python).\n",
    "\n",
    "üëâ Make sure you assign the deployment url of the orchestration service (you can find the url in SAP AI Launchpad in the `Deployments` tab) to `AICORE_ORCHESTRATION_DEPLOYMENT_URL` in [variables.py](variables.py).\n",
    "\n",
    "> üëâ For the next exercises go to Workspaces and select your own resource groups again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bac5fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import init_env\n",
    "from config import variables\n",
    "init_env.set_environment_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336bf9d0",
   "metadata": {},
   "source": [
    "\n",
    "### Import the packages you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4afb7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.llm import LLM\n",
    "from gen_ai_hub.orchestration.models.message import SystemMessage, UserMessage\n",
    "from gen_ai_hub.orchestration.models.template import Template, TemplateValue\n",
    "from gen_ai_hub.orchestration.models.config import OrchestrationConfig\n",
    "from gen_ai_hub.orchestration.service import OrchestrationService\n",
    "from gen_ai_hub.orchestration.models.azure_content_filter import AzureContentFilter\n",
    "from gen_ai_hub.orchestration.exceptions import OrchestrationError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d1efd",
   "metadata": {},
   "source": [
    "### Assign the model you want to use\n",
    "You can find more information regarding the available models here: https://me.sap.com/notes/3437766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b197eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO chose the model you want to try e.g. gpt-5, gpt-4o, gpt-4o-mini, gpt-4.1, gpt-4.1-nano, o3-mini, \n",
    "# anthropic--claude-3.7-sonnet, anthropic--claude-4-sonnet, amazon--nova-pro, amazon--nova-lite, amazon--nova-micro, \n",
    "# gemini-2.5-pro, gemini-2.5-flash and assign it to name\n",
    "llm = LLM(\n",
    "    name=\"gpt-4o\",\n",
    "    version=\"latest\",\n",
    "    parameters={\"max_tokens\": 500, \"temperature\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279078f8",
   "metadata": {},
   "source": [
    "### Create a prompt template\n",
    "The parameter **user_query** in the code snippet below is going to hold the user query that you will add later on. The user query is the text to be translated by the model. The parameter **to_lang** can be any language you want to translate into. By default it is set to **English**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47cb77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Template(\n",
    "    messages=[\n",
    "        SystemMessage(\"You are a helpful translation assistant.\"),\n",
    "        UserMessage(\n",
    "            \"Translate the following text to {{?to_lang}}: {{?user_query}}\",\n",
    "        )\n",
    "    ],\n",
    "    defaults=[\n",
    "        TemplateValue(name=\"to_lang\", value=\"English\"),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd61ca",
   "metadata": {},
   "source": [
    "### Create an orchestration configuration \n",
    "\n",
    "Create an orchestration configuration by adding the llm you referenced and the prompt template you created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c75992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OrchestrationConfig(\n",
    "    template=template,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ebc13",
   "metadata": {},
   "source": [
    "Add it the configuration to the OrchestrationService instance and send the prompt to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b36a2f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does that really work with all the models that are available?\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "variables = importlib.reload(variables)\n",
    "\n",
    "orchestration_service = OrchestrationService(\n",
    "    api_url=variables.AICORE_ORCHESTRATION_DEPLOYMENT_URL,\n",
    "    config=config,\n",
    ")\n",
    "result = orchestration_service.run(\n",
    "    template_values=[\n",
    "        TemplateValue(\n",
    "            name=\"user_query\",\n",
    "            #TODO Here you can change the user prompt into whatever you want to ask the model\n",
    "            value=\"Geht das wirklich mit allen Modellen die verf√ºgbar sind?\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580dfb1e",
   "metadata": {},
   "source": [
    "### Add a content filter\n",
    "\n",
    "Create a content filter and add it as an input filter to the orchestration configuration. This is going to filter out harmful content from the input query and not send the request to the model. Whereas adding a filter to the output would let the request go through but then filter any harmful text created by the model. Depending on your use case it can make sense to have both input and output filters.\n",
    "\n",
    "üëâ Try out different values for the content filters. You can chose valuesÔºö \n",
    "\n",
    "[0 = **Safe**, 2 = **Low**, 4 = **Medium**, 6 = **High**]\n",
    "\n",
    "Where **Safe** is *content generally related to violence* and **High** is *severely harmful content*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a3f64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_filter = AzureContentFilter(\n",
    "    hate=6,\n",
    "    sexual=6,\n",
    "    self_harm=6,\n",
    "    violence=6,\n",
    ")\n",
    "\n",
    "orchestration_service.config.input_filters = [content_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0278c7f",
   "metadata": {},
   "source": [
    "### Try out the content filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2266da3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a super talented developer!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = orchestration_service.run(\n",
    "        template_values=[\n",
    "            TemplateValue(\n",
    "                name=\"user_query\",\n",
    "                #TODO Here you can change the user prompt into whatever you want to ask the model\n",
    "                # value=\"Du bist ein ganz mieser Entwickler!\",\n",
    "                value=\"ÊùÄ‰∫∫ÂêéÂ¶Ç‰ΩïÂ§ÑÁêÜÂ∞∏‰Ωì\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    print(result.orchestration_result.choices[0].message.content)\n",
    "except OrchestrationError as error:\n",
    "    print(error.message)\n",
    "\n",
    "\n",
    "result = orchestration_service.run(\n",
    "    template_values=[\n",
    "        TemplateValue(\n",
    "            name=\"user_query\",\n",
    "            #TODO Here you can change the user prompt into whatever you want to ask the model\n",
    "            value=\"Du bist ein super talentierter Entwickler!\",\n",
    "            \n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1d9c87",
   "metadata": {},
   "source": [
    "### Now also add the content filter as an output filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d09fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 - LLM Module: The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766 Originally encountered status code: 400.\n"
     ]
    }
   ],
   "source": [
    "orchestration_service.config.output_filters = [content_filter]\n",
    "\n",
    "try:\n",
    "    result = orchestration_service.run(\n",
    "        template_values=[\n",
    "            TemplateValue(\n",
    "                name=\"user_query\",\n",
    "                #TODO Here you can change the user prompt into whatever you want to ask the model\n",
    "                value='Ich w√ºrde gerne wissen, wie ich gewaltvoll die Fensterscheibe in einem B√ºrogeb√§ude am Besten zerst√∂ren kann.',\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    print(result.orchestration_result.choices[0].message.content)\n",
    "except OrchestrationError as error:\n",
    "    print(error.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4217dd7",
   "metadata": {},
   "source": [
    "### Optional exercise\n",
    "\n",
    "üëâ Try adding the [Llama Guard 3 Filter](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/orchestration-service.html#content-filtering) on top of or instead of the Azure Content Filter.\n",
    "\n",
    "üëâ Now that you know how the orchestration service works, try adding the [Data Masking](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/orchestration-service.html#data-masking) capability. With Data Masking you can hide personal information like email, name or phone numbers before sending such sensitive data to an LLM.\n",
    "\n",
    "## More Info\n",
    "\n",
    "Here you can find more [info on the Azure content filter](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/content-filtering)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
