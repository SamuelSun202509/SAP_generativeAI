{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d17dbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource group is set to: default\n"
     ]
    }
   ],
   "source": [
    "from config import init_env\n",
    "from config import variables\n",
    "import importlib\n",
    "variables = importlib.reload(variables)\n",
    "\n",
    "# TODO: You need to specify which model you want to use. In this case we are directing our prompt\n",
    "# to the openAI API directly so you need to pick one of the GPT models. Make sure the model is actually deployed\n",
    "# in genAI Hub. You might also want to chose a model that can also process images here already. \n",
    "# E.g. 'gpt-4.1-mini'\n",
    "MODEL_NAME = 'gpt-4o'\n",
    "\n",
    "# Do not modify the `assert` line below\n",
    "assert MODEL_NAME!='', \"\"\"You should change the variable `MODEL_NAME` with the name of your deployed model (like 'gpt-4o-mini') first!\"\"\"\n",
    "\n",
    "init_env.set_environment_variables()\n",
    "# Do not modify the `assert` line below \n",
    "assert variables.RESOURCE_GROUP!='', \"\"\"You should change the value assigned to the `RESOURCE_GROUP` in the `variables.py` file to your own resource group first!\"\"\"\n",
    "\n",
    "print(f\"Resource group is set to: {variables.RESOURCE_GROUP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7dfa5",
   "metadata": {},
   "source": [
    "[*Tutorial01 - Prompt LLMs in the generative AI hub in SAP AI Core & Launchpad*](https://developers.sap.com/tutorials/ai-core-generative-ai.html#cd7f285b-9a2e-4218-9bbe-b334efaca7e9)\n",
    "## Prompt LLMs in the generative AI hub in SAP AI Core & Launchpad\n",
    "Understand the principles of prompt engineering and learn to create effective prompts for AI models like ChatGPT. Prompt engineering is crucial for obtaining desired outputs from AI models. It helps in guiding the model's response in a particular direction.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c401f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "from config import get_token # Get the access token for the checking APIs below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7d2fc",
   "metadata": {},
   "source": [
    "### Checking for foundation-models scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7547394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## API data\n",
    "endpoint = \"/v2/lm/scenarios\" #set the API specific parameters\n",
    "url =  os.environ[\"AICORE_BASE_URL\"]+endpoint\n",
    "\n",
    "# Set header\n",
    "headers = {\n",
    "  'AI-Resource-Group': variables.RESOURCE_GROUP,\n",
    "  'Content-Type': 'application/json',\n",
    "  'Authorization': os.getenv('AICORE_CLIENT_TOKEN')\n",
    "}\n",
    "# Run API\n",
    "api_response = requests.request(\n",
    "  \"GET\", \n",
    "  url, \n",
    "  headers=headers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0002d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario 1:\n",
      "createdAt: 2023-11-21T09:41:52+00:00\n",
      "description: AI Core Global Scenario for LLM Access\n",
      "id: foundation-models\n",
      "labels: [{'key': 'scenarios.ai.sap.com/llm', 'value': 'true'}]\n",
      "modifiedAt: 2025-10-16T09:59:39+00:00\n",
      "name: foundation-models\n",
      "\n",
      "Scenario 2:\n",
      "createdAt: 2024-08-02T06:14:21+00:00\n",
      "description: AI Core Global Scenario for the Orchestration Service\n",
      "id: orchestration\n",
      "modifiedAt: 2024-08-02T06:14:21+00:00\n",
      "name: orchestration\n",
      "\n",
      "Scenario 3:\n",
      "createdAt: 2025-06-03T18:44:50+00:00\n",
      "description: Introduction to SAP AI Core\n",
      "id: learning46\n",
      "modifiedAt: 2025-06-03T18:44:50+00:00\n",
      "name: Tutorial46\n",
      "\n",
      "Scenario 4:\n",
      "createdAt: 2025-06-03T19:16:50+00:00\n",
      "description: Introduction to SAP AI Core - ACS\n",
      "id: learning-acs\n",
      "modifiedAt: 2025-06-03T19:16:50+00:00\n",
      "name: Tutorial - ACS\n",
      "\n",
      "Scenario 5:\n",
      "createdAt: 2025-06-04T08:03:23+00:00\n",
      "description: AI Core global scenario for the generative AI hub evaluation service\n",
      "id: genai-evaluations\n",
      "modifiedAt: 2025-08-29T07:57:15+00:00\n",
      "name: genai-evaluations\n",
      "\n",
      "Scenario 6:\n",
      "createdAt: 2025-06-04T12:18:11+00:00\n",
      "description: Introduction to SAP AI Core\n",
      "id: learning\n",
      "modifiedAt: 2025-06-04T12:18:11+00:00\n",
      "name: Tutorial\n",
      "\n",
      "Scenario 7:\n",
      "createdAt: 2025-06-10T17:01:20+00:00\n",
      "description: Introduction to SAP AI Core\n",
      "id: learning-4321\n",
      "modifiedAt: 2025-06-10T17:01:20+00:00\n",
      "name: Tutorial-4321\n",
      "\n",
      "Scenario 8:\n",
      "createdAt: 2025-06-18T11:00:48+00:00\n",
      "description: DQN model into SAP AI Core\n",
      "id: learning-datalines\n",
      "modifiedAt: 2025-06-18T11:00:48+00:00\n",
      "name: DQN\n",
      "\n",
      "Scenario 9:\n",
      "createdAt: 2025-06-26T08:24:38+00:00\n",
      "description: Introduction to SAP AI Core\n",
      "id: SS-learning-01\n",
      "modifiedAt: 2025-06-26T08:24:38+00:00\n",
      "name: SS-Tutorial-01\n",
      "\n",
      "Scenario 10:\n",
      "createdAt: 2025-06-30T17:56:50+00:00\n",
      "description: Tutorial to add custom code to SAP AI Core\n",
      "id: learning-code\n",
      "modifiedAt: 2025-06-30T17:56:50+00:00\n",
      "name: Code (Tutorial)\n",
      "\n",
      "Scenario 11:\n",
      "createdAt: 2025-07-08T07:10:40+00:00\n",
      "description: Tutorial to add custom code to SAP AI Core\n",
      "id: SS-learning-02\n",
      "modifiedAt: 2025-07-08T07:10:40+00:00\n",
      "name: SS-Tutorial-01\n",
      "\n",
      "Scenario 12:\n",
      "createdAt: 2025-09-05T02:27:48+00:00\n",
      "description: Introduction to SAP AI Core\n",
      "id: learning_SS010\n",
      "modifiedAt: 2025-09-05T02:27:48+00:00\n",
      "name: Tutorial_scenarios_SS010\n",
      "\n",
      "Scenario 13:\n",
      "createdAt: 2025-09-10T08:46:29.672000+00:00\n",
      "id: MyScenario\n",
      "modifiedAt: 2025-09-19T02:59:22.947000+00:00\n",
      "name: MyScenario\n",
      "\n",
      "Scenario 14:\n",
      "createdAt: 2025-10-30T03:24:05+00:00\n",
      "description: AI Core global scenario for the generative AI hub prompt optimization service\n",
      "id: genai-optimizations\n",
      "modifiedAt: 2025-10-30T03:24:05+00:00\n",
      "name: genai-optimizations\n"
     ]
    }
   ],
   "source": [
    "# Print result\n",
    "data = json.loads(api_response.text)\n",
    "# # Access the configuration list and print each key value details\n",
    "for i, scenario in enumerate(data[\"resources\"], start=1):\n",
    "    print(f\"\\nScenario {i}:\")\n",
    "    for key, value in scenario.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3662583",
   "metadata": {},
   "source": [
    "### Checking for configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b53638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## API data\n",
    "endpoint = \"/v2/lm/configurations\" #set the API specific parameters\n",
    "url =  os.environ[\"AICORE_BASE_URL\"]+endpoint\n",
    "\n",
    "# Set header\n",
    "headers = {\n",
    "  'AI-Resource-Group': variables.RESOURCE_GROUP,\n",
    "  'Content-Type': 'application/json',\n",
    "  'Authorization': os.getenv('AICORE_CLIENT_TOKEN')\n",
    "}\n",
    "# Run API\n",
    "api_response = requests.request(\n",
    "  \"GET\", \n",
    "  url, \n",
    "  headers=headers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b556a7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration 1:\n",
      "id: caaa9e93-6ecd-44aa-a660-491d5beeabfa\n",
      "createdAt: 2025-10-01T05:04:00Z\n",
      "name: gpt-5-mini_autogenerated\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-5-mini'}, {'key': 'modelVersion', 'value': '2025-08-07'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 2:\n",
      "id: ce1241ca-ae7b-43c5-9d80-e65535e60afa\n",
      "createdAt: 2025-09-29T09:24:25Z\n",
      "name: cortex-1\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 3:\n",
      "id: 7c15b99e-9eea-4ae8-83ba-b1e5cf83f115\n",
      "createdAt: 2025-09-22T03:15:07Z\n",
      "name: text-embedding-3-large-ss01\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'text-embedding-3-large'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 4:\n",
      "id: 796aca2d-d225-4ffe-9b0b-e83e3b3f7f9f\n",
      "createdAt: 2025-09-16T02:19:08Z\n",
      "name: config_ss02_orchestration\n",
      "executableId: orchestration\n",
      "scenarioId: orchestration\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 5:\n",
      "id: 43b0b78c-1555-42bd-8928-4debe4f6d4fb\n",
      "createdAt: 2025-09-16T02:17:09Z\n",
      "name: config_ss02_orchestration\n",
      "executableId: orchestration\n",
      "scenarioId: orchestration\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 6:\n",
      "id: 01d20309-52af-42b2-a368-1ea69b79aaab\n",
      "createdAt: 2025-09-16T02:14:49Z\n",
      "name: config_ss02_orchestration\n",
      "executableId: orchestration\n",
      "scenarioId: orchestration\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 7:\n",
      "id: b4a884be-1a61-46fa-a3ed-3f8ebd5c1173\n",
      "createdAt: 2025-09-16T02:14:02Z\n",
      "name: config_ss02_orchestration\n",
      "executableId: orchestration\n",
      "scenarioId: orchestration\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 8:\n",
      "id: 4565c116-f060-4b56-b7e5-791b23a9530d\n",
      "createdAt: 2025-09-15T02:23:11Z\n",
      "name: gpt-4o-openai-ss01\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 9:\n",
      "id: 98d42681-6799-4999-b0ca-b04729696a36\n",
      "createdAt: 2025-09-08T05:38:23Z\n",
      "name: config_ss01_orchestration\n",
      "executableId: orchestration\n",
      "scenarioId: orchestration\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 10:\n",
      "id: c51987eb-8d21-4626-a1a2-8fb2a085b43c\n",
      "createdAt: 2025-09-05T05:29:28Z\n",
      "name: hello-pipeline-conf_ss010\n",
      "executableId: first-pipeline-ss-010\n",
      "scenarioId: learning_SS010\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 11:\n",
      "id: 28121ef3-f8e6-4824-a9e7-24ec343b44a4\n",
      "createdAt: 2025-09-05T05:28:09Z\n",
      "name: hello-pipeline-conf_ss010\n",
      "executableId: first-pipeline-ss-010\n",
      "scenarioId: learning_SS010\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 12:\n",
      "id: 30708a48-46af-4f5a-941c-361ca76713d7\n",
      "createdAt: 2025-08-02T04:16:21Z\n",
      "name: defaultOrchestrationConfig\n",
      "executableId: orchestration\n",
      "scenarioId: orchestration\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 13:\n",
      "id: 0ff8999f-a22c-45bd-8e8b-85eaff8ef985\n",
      "createdAt: 2025-07-20T10:34:51Z\n",
      "name: gtp-4-32k-config\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o-mini'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 14:\n",
      "id: 62fd321f-3d3d-4590-be4e-a0102977cf48\n",
      "createdAt: 2025-07-08T07:30:00Z\n",
      "name: ss02-Configuration-CodePipeline\n",
      "executableId: ss01-code-pipeline\n",
      "scenarioId: SS-learning-02\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 15:\n",
      "id: 7193ef7b-3487-49af-9063-cbf8713672a9\n",
      "createdAt: 2025-07-01T02:25:27Z\n",
      "name: Hello Pipeline-SSConfiguration01\n",
      "executableId: ss-first-pipeline\n",
      "scenarioId: SS-learning-01\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 16:\n",
      "id: 4030de5c-c098-47e7-bcd6-7a3999b2a95e\n",
      "createdAt: 2025-06-30T18:00:07Z\n",
      "name: code-workflow-slp4ca\n",
      "executableId: code-pipeline-slp4ca\n",
      "scenarioId: learning-code\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 17:\n",
      "id: d9d3010f-0b06-450b-8f43-3ab8d43e3263\n",
      "createdAt: 2025-06-25T09:51:22Z\n",
      "name: code-pipeline-s1\n",
      "executableId: code-pipeline-s123\n",
      "scenarioId: learning-code\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 18:\n",
      "id: d91ee1e5-ed89-477b-8d2d-b4f4df995ffa\n",
      "createdAt: 2025-06-25T06:41:54Z\n",
      "name: Hello Pipeline-Sabcde-exe\n",
      "executableId: first-pipeline-s1234\n",
      "scenarioId: learning\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 19:\n",
      "id: d7ce8a2d-09fd-44f2-9dd5-294ec0eb180b\n",
      "createdAt: 2025-06-25T06:17:32Z\n",
      "name: Hello Pipeline-Sabcd-exe\n",
      "executableId: first-pipeline-s1234\n",
      "scenarioId: learning\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 20:\n",
      "id: 6adc3158-a2b0-4624-b995-13779d2c0018\n",
      "createdAt: 2025-06-19T09:07:30Z\n",
      "name: Hello Pipeline-Sabc-exe\n",
      "executableId: first-pipeline-s1234\n",
      "scenarioId: learning\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 21:\n",
      "id: 8112fa53-1f28-4f02-aa11-4c55107e493f\n",
      "createdAt: 2025-06-18T11:07:17Z\n",
      "name: maintenance_sche_02\n",
      "executableId: maintenance-pipeline\n",
      "scenarioId: learning-datalines\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 22:\n",
      "id: 9e856a97-8ff4-4858-afe0-9a74390da077\n",
      "createdAt: 2025-06-18T08:35:57Z\n",
      "name: maintenace-sche-01\n",
      "executableId: maintenance-pipeline\n",
      "scenarioId: learning-code\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 23:\n",
      "id: 5d6c2c8d-2013-48e1-9f70-82e967f256db\n",
      "createdAt: 2025-06-12T14:33:22Z\n",
      "name: text-embedding-3-large_autogenerated\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'text-embedding-3-large'}, {'key': 'modelVersion', 'value': '1'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 24:\n",
      "id: eee9981e-02ba-43f3-b051-07dbce2fdba0\n",
      "createdAt: 2025-06-12T11:05:06Z\n",
      "name: gemini-2.0-flash_autogenerated\n",
      "executableId: gcp-vertexai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gemini-2.0-flash'}, {'key': 'modelVersion', 'value': '001'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 25:\n",
      "id: 146a2393-47e4-49ad-8f3d-7d615d0fd8f7\n",
      "createdAt: 2025-06-10T17:18:53Z\n",
      "name: hello-pipeline-confg\n",
      "executableId: first-pipeline\n",
      "scenarioId: learning\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 26:\n",
      "id: f86b2ac2-89cf-4744-b103-b24c96590026\n",
      "createdAt: 2025-06-10T03:28:05Z\n",
      "name: Hello_worldof\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o-mini'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 27:\n",
      "id: 76807a73-19e8-4ba6-a447-6d100c5d999c\n",
      "createdAt: 2025-06-10T03:28:04Z\n",
      "name: Hello_worldof\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o-mini'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 28:\n",
      "id: e33d84da-8cae-4f8d-afc3-a81c35403b06\n",
      "createdAt: 2025-06-05T02:50:52Z\n",
      "name: TestAI\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o-mini'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 29:\n",
      "id: 44913d00-b1e4-4811-b540-2a05d3d037b4\n",
      "createdAt: 2025-06-04T10:58:40Z\n",
      "name: Hello World\n",
      "executableId: first-pipeline46\n",
      "scenarioId: learning\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 30:\n",
      "id: 79948798-0433-4484-9fb5-49d183365174\n",
      "createdAt: 2025-06-03T19:45:01Z\n",
      "name: hello-pipeline-configuration\n",
      "executableId: first-pipeline46\n",
      "scenarioId: learning\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 31:\n",
      "id: 9c0111ab-f556-4b61-8ebc-2cbe57255fa8\n",
      "createdAt: 2025-06-03T19:45:00Z\n",
      "name: hello-pipeline-configuration\n",
      "executableId: first-pipeline46\n",
      "scenarioId: learning\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 32:\n",
      "id: 15979868-4b86-4421-8282-c4691e2afd8b\n",
      "createdAt: 2025-06-03T19:35:36Z\n",
      "name: hello-pipeline-conf-acs\n",
      "executableId: first-pipeline-acs\n",
      "scenarioId: learning-acs\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 33:\n",
      "id: 75aee553-24a2-4d08-ab84-d23cc6c1a490\n",
      "createdAt: 2025-05-28T12:28:29Z\n",
      "name: HUQ3HC_DEMO\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o-mini'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 34:\n",
      "id: c2f24ea3-9866-4eff-8488-dfafa9d9b497\n",
      "createdAt: 2025-05-28T12:28:19Z\n",
      "name: HUQ3HC_DEMO\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o-mini'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 35:\n",
      "id: 8cecd483-0695-4fd6-87c5-b41359fa1627\n",
      "createdAt: 2025-05-22T18:18:33Z\n",
      "name: orchestration\n",
      "executableId: orchestration\n",
      "scenarioId: orchestration\n",
      "parameterBindings: [{'key': 'modelFilterList', 'value': 'null'}, {'key': 'modelFilterListType', 'value': 'allow'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration 36:\n",
      "id: 2b2f911e-59a0-46a2-9419-3fd9db293a25\n",
      "createdAt: 2025-05-14T09:19:06Z\n",
      "name: defaultOrchestrationConfig\n",
      "executableId: orchestration\n",
      "scenarioId: orchestration\n",
      "parameterBindings: []\n",
      "inputArtifactBindings: []\n"
     ]
    }
   ],
   "source": [
    "# Print result\n",
    "data = json.loads(api_response.text)\n",
    "# # Access the configuration list and print each key value details\n",
    "for i, config in enumerate(data[\"resources\"], start=1):\n",
    "    print(f\"\\nConfiguration {i}:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4746b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 configuration(s):\n",
      "\n",
      "Configuration #1:\n",
      "id: caaa9e93-6ecd-44aa-a660-491d5beeabfa\n",
      "createdAt: 2025-10-01T05:04:00Z\n",
      "name: gpt-5-mini_autogenerated\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-5-mini'}, {'key': 'modelVersion', 'value': '2025-08-07'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration #2:\n",
      "id: ce1241ca-ae7b-43c5-9d80-e65535e60afa\n",
      "createdAt: 2025-09-29T09:24:25Z\n",
      "name: cortex-1\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration #3:\n",
      "id: 7c15b99e-9eea-4ae8-83ba-b1e5cf83f115\n",
      "createdAt: 2025-09-22T03:15:07Z\n",
      "name: text-embedding-3-large-ss01\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'text-embedding-3-large'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration #4:\n",
      "id: 4565c116-f060-4b56-b7e5-791b23a9530d\n",
      "createdAt: 2025-09-15T02:23:11Z\n",
      "name: gpt-4o-openai-ss01\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration #5:\n",
      "id: 0ff8999f-a22c-45bd-8e8b-85eaff8ef985\n",
      "createdAt: 2025-07-20T10:34:51Z\n",
      "name: gtp-4-32k-config\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o-mini'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration #6:\n",
      "id: 5d6c2c8d-2013-48e1-9f70-82e967f256db\n",
      "createdAt: 2025-06-12T14:33:22Z\n",
      "name: text-embedding-3-large_autogenerated\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'text-embedding-3-large'}, {'key': 'modelVersion', 'value': '1'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration #7:\n",
      "id: eee9981e-02ba-43f3-b051-07dbce2fdba0\n",
      "createdAt: 2025-06-12T11:05:06Z\n",
      "name: gemini-2.0-flash_autogenerated\n",
      "executableId: gcp-vertexai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gemini-2.0-flash'}, {'key': 'modelVersion', 'value': '001'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration #8:\n",
      "id: f86b2ac2-89cf-4744-b103-b24c96590026\n",
      "createdAt: 2025-06-10T03:28:05Z\n",
      "name: Hello_worldof\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o-mini'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration #9:\n",
      "id: 76807a73-19e8-4ba6-a447-6d100c5d999c\n",
      "createdAt: 2025-06-10T03:28:04Z\n",
      "name: Hello_worldof\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o-mini'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration #10:\n",
      "id: e33d84da-8cae-4f8d-afc3-a81c35403b06\n",
      "createdAt: 2025-06-05T02:50:52Z\n",
      "name: TestAI\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o-mini'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration #11:\n",
      "id: 75aee553-24a2-4d08-ab84-d23cc6c1a490\n",
      "createdAt: 2025-05-28T12:28:29Z\n",
      "name: HUQ3HC_DEMO\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o-mini'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n",
      "\n",
      "Configuration #12:\n",
      "id: c2f24ea3-9866-4eff-8488-dfafa9d9b497\n",
      "createdAt: 2025-05-28T12:28:19Z\n",
      "name: HUQ3HC_DEMO\n",
      "executableId: azure-openai\n",
      "scenarioId: foundation-models\n",
      "parameterBindings: [{'key': 'modelName', 'value': 'gpt-4o-mini'}, {'key': 'modelVersion', 'value': 'latest'}]\n",
      "inputArtifactBindings: []\n"
     ]
    }
   ],
   "source": [
    "# find all the configuration IDs of foundation models using the scenarioId\n",
    "\n",
    "search_id = \"foundation-models\"  # scenarioId for foundation models\n",
    "\n",
    "matches = [config for config in data.get(\"resources\", []) if config.get(\"scenarioId\") == search_id]\n",
    "\n",
    "if matches:\n",
    "    print(f\"Found {len(matches)} configuration(s):\")\n",
    "    for idx, config in enumerate(matches, start=1):\n",
    "        print(f\"\\nConfiguration #{idx}:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"No configuration found for scenarioId:\", search_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d252c7",
   "metadata": {},
   "source": [
    "### Checking for deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a690797",
   "metadata": {},
   "outputs": [],
   "source": [
    "## API data\n",
    "endpoint = \"/v2/lm/deployments\" #set the API specific parameters\n",
    "url =  os.environ[\"AICORE_BASE_URL\"]+endpoint\n",
    "\n",
    "# Set header\n",
    "headers = {\n",
    "  'AI-Resource-Group': variables.RESOURCE_GROUP,\n",
    "  'Content-Type': 'application/json',\n",
    "  'Authorization': os.getenv('AICORE_CLIENT_TOKEN')\n",
    "}\n",
    "# Set body\n",
    "\n",
    "# Run API\n",
    "api_response = requests.request(\n",
    "  \"GET\", \n",
    "  url, \n",
    "  headers=headers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2545f932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "deployment 1:\n",
      "id: d4dbb11884ef91e3\n",
      "createdAt: 2025-10-01T05:04:05Z\n",
      "modifiedAt: 2025-11-13T03:39:08Z\n",
      "status: RUNNING\n",
      "details: {'resources': {'backendDetails': {'model': {'name': 'gpt-5-mini', 'version': '2025-08-07'}}, 'backend_details': {'model': {'name': 'gpt-5-mini', 'version': '2025-08-07'}}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: foundation-models\n",
      "configurationId: caaa9e93-6ecd-44aa-a660-491d5beeabfa\n",
      "latestRunningConfigurationId: caaa9e93-6ecd-44aa-a660-491d5beeabfa\n",
      "lastOperation: CREATE\n",
      "targetStatus: RUNNING\n",
      "submissionTime: 2025-10-01T05:10:24Z\n",
      "startTime: 2025-10-01T05:13:05Z\n",
      "configurationName: gpt-5-mini_autogenerated\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d4dbb11884ef91e3\n",
      "\n",
      "deployment 2:\n",
      "id: d07b68722141e03d\n",
      "createdAt: 2025-09-29T09:32:49Z\n",
      "modifiedAt: 2025-11-13T03:39:08Z\n",
      "status: RUNNING\n",
      "details: {'resources': {'backendDetails': {'model': {'name': 'gpt-4o', 'version': 'latest'}}, 'backend_details': {'model': {'name': 'gpt-4o', 'version': 'latest'}}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: foundation-models\n",
      "configurationId: ce1241ca-ae7b-43c5-9d80-e65535e60afa\n",
      "latestRunningConfigurationId: ce1241ca-ae7b-43c5-9d80-e65535e60afa\n",
      "lastOperation: CREATE\n",
      "targetStatus: RUNNING\n",
      "submissionTime: 2025-09-29T09:35:53Z\n",
      "startTime: 2025-09-29T09:38:07Z\n",
      "configurationName: cortex-1\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d07b68722141e03d\n",
      "\n",
      "deployment 3:\n",
      "id: d12a770eac7c91c9\n",
      "createdAt: 2025-09-22T03:19:50Z\n",
      "modifiedAt: 2025-11-13T03:39:08Z\n",
      "status: RUNNING\n",
      "details: {'resources': {'backendDetails': {'model': {'name': 'text-embedding-3-large', 'version': 'latest'}}, 'backend_details': {'model': {'name': 'text-embedding-3-large', 'version': 'latest'}}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: foundation-models\n",
      "configurationId: 7c15b99e-9eea-4ae8-83ba-b1e5cf83f115\n",
      "latestRunningConfigurationId: 7c15b99e-9eea-4ae8-83ba-b1e5cf83f115\n",
      "lastOperation: CREATE\n",
      "targetStatus: RUNNING\n",
      "submissionTime: 2025-09-22T03:25:11Z\n",
      "startTime: 2025-09-22T03:29:40Z\n",
      "configurationName: text-embedding-3-large-ss01\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d12a770eac7c91c9\n",
      "\n",
      "deployment 4:\n",
      "id: d02c3ee65a3a9f5c\n",
      "createdAt: 2025-09-16T02:21:08Z\n",
      "modifiedAt: 2025-11-13T03:39:08Z\n",
      "status: RUNNING\n",
      "details: {'resources': {'backendDetails': {}, 'backend_details': {}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: orchestration\n",
      "configurationId: 796aca2d-d225-4ffe-9b0b-e83e3b3f7f9f\n",
      "latestRunningConfigurationId: 796aca2d-d225-4ffe-9b0b-e83e3b3f7f9f\n",
      "lastOperation: CREATE\n",
      "targetStatus: RUNNING\n",
      "submissionTime: 2025-09-16T02:23:59Z\n",
      "startTime: 2025-09-16T02:35:25Z\n",
      "configurationName: config_ss02_orchestration\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d02c3ee65a3a9f5c\n",
      "\n",
      "deployment 5:\n",
      "id: deb3f11c695add12\n",
      "createdAt: 2025-09-15T03:12:08Z\n",
      "modifiedAt: 2025-11-13T03:39:08Z\n",
      "status: RUNNING\n",
      "details: {'resources': {'backendDetails': {'model': {'name': 'gpt-4o', 'version': 'latest'}}, 'backend_details': {'model': {'name': 'gpt-4o', 'version': 'latest'}}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: foundation-models\n",
      "configurationId: 4565c116-f060-4b56-b7e5-791b23a9530d\n",
      "latestRunningConfigurationId: 4565c116-f060-4b56-b7e5-791b23a9530d\n",
      "lastOperation: CREATE\n",
      "targetStatus: RUNNING\n",
      "submissionTime: 2025-09-15T03:13:04Z\n",
      "startTime: 2025-09-15T03:20:54Z\n",
      "configurationName: gpt-4o-openai-ss01\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/deb3f11c695add12\n",
      "\n",
      "deployment 6:\n",
      "id: dad050d1d8d50520\n",
      "createdAt: 2025-09-08T05:46:54Z\n",
      "modifiedAt: 2025-11-13T03:39:08Z\n",
      "status: RUNNING\n",
      "details: {'resources': {'backendDetails': {}, 'backend_details': {}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: orchestration\n",
      "configurationId: 98d42681-6799-4999-b0ca-b04729696a36\n",
      "latestRunningConfigurationId: 98d42681-6799-4999-b0ca-b04729696a36\n",
      "lastOperation: CREATE\n",
      "targetStatus: RUNNING\n",
      "submissionTime: 2025-09-08T05:48:43Z\n",
      "startTime: 2025-09-08T06:02:06Z\n",
      "configurationName: config_ss01_orchestration\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dad050d1d8d50520\n",
      "\n",
      "deployment 7:\n",
      "id: d0db94a47c432560\n",
      "createdAt: 2025-08-02T04:16:24Z\n",
      "modifiedAt: 2025-08-07T07:41:32Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {}, 'backend_details': {}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: orchestration\n",
      "configurationId: 30708a48-46af-4f5a-941c-361ca76713d7\n",
      "latestRunningConfigurationId: 30708a48-46af-4f5a-941c-361ca76713d7\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-08-02T04:19:23Z\n",
      "startTime: 2025-08-02T04:28:19Z\n",
      "completionTime: 2025-08-07T07:50:01Z\n",
      "configurationName: defaultOrchestrationConfig\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d0db94a47c432560\n",
      "\n",
      "deployment 8:\n",
      "id: d0f04d2e99338c1b\n",
      "createdAt: 2025-07-29T03:59:23Z\n",
      "modifiedAt: 2025-08-07T07:41:32Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {'model': {'name': 'gpt-4o-mini', 'version': 'latest'}}, 'backend_details': {'model': {'name': 'gpt-4o-mini', 'version': 'latest'}}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: foundation-models\n",
      "configurationId: 75aee553-24a2-4d08-ab84-d23cc6c1a490\n",
      "latestRunningConfigurationId: 75aee553-24a2-4d08-ab84-d23cc6c1a490\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-07-29T04:01:23Z\n",
      "startTime: 2025-07-29T04:03:43Z\n",
      "completionTime: 2025-08-07T07:53:48Z\n",
      "configurationName: HUQ3HC_DEMO\n",
      "ttl: 18d\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d0f04d2e99338c1b\n",
      "\n",
      "deployment 9:\n",
      "id: da6f4d1ca46891f8\n",
      "createdAt: 2025-07-29T03:59:23Z\n",
      "modifiedAt: 2025-08-07T07:41:35Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {'model': {'name': 'gpt-4o-mini', 'version': 'latest'}}, 'backend_details': {'model': {'name': 'gpt-4o-mini', 'version': 'latest'}}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: foundation-models\n",
      "configurationId: 75aee553-24a2-4d08-ab84-d23cc6c1a490\n",
      "latestRunningConfigurationId: 75aee553-24a2-4d08-ab84-d23cc6c1a490\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-07-29T04:01:23Z\n",
      "startTime: 2025-07-29T04:03:43Z\n",
      "completionTime: 2025-08-07T07:53:48Z\n",
      "configurationName: HUQ3HC_DEMO\n",
      "ttl: 18d\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/da6f4d1ca46891f8\n",
      "\n",
      "deployment 10:\n",
      "id: da8afe77ced186bd\n",
      "createdAt: 2025-07-28T12:34:12Z\n",
      "modifiedAt: 2025-08-07T07:41:32Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {'model': {'name': 'gpt-4o-mini', 'version': 'latest'}}, 'backend_details': {'model': {'name': 'gpt-4o-mini', 'version': 'latest'}}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: foundation-models\n",
      "configurationId: 0ff8999f-a22c-45bd-8e8b-85eaff8ef985\n",
      "latestRunningConfigurationId: 0ff8999f-a22c-45bd-8e8b-85eaff8ef985\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-07-28T12:56:34Z\n",
      "startTime: 2025-07-28T12:59:12Z\n",
      "completionTime: 2025-08-07T07:53:48Z\n",
      "configurationName: gtp-4-32k-config\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/da8afe77ced186bd\n",
      "\n",
      "deployment 11:\n",
      "id: d68ebf1f0ff7fc07\n",
      "createdAt: 2025-07-20T10:35:35Z\n",
      "modifiedAt: 2025-07-28T09:07:25Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {'model': {'name': 'gpt-4o-mini', 'version': 'latest'}}, 'backend_details': {'model': {'name': 'gpt-4o-mini', 'version': 'latest'}}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: foundation-models\n",
      "configurationId: 0ff8999f-a22c-45bd-8e8b-85eaff8ef985\n",
      "latestRunningConfigurationId: 0ff8999f-a22c-45bd-8e8b-85eaff8ef985\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-07-20T10:42:29Z\n",
      "startTime: 2025-07-20T10:48:22Z\n",
      "completionTime: 2025-07-28T09:10:07Z\n",
      "configurationName: gtp-4-32k-config\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d68ebf1f0ff7fc07\n",
      "\n",
      "deployment 12:\n",
      "id: d30e94d179443d52\n",
      "createdAt: 2025-06-24T06:46:14Z\n",
      "modifiedAt: 2025-08-07T07:41:33Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {'predictor': {'resourcePlan': 'starter'}}, 'backend_details': {'predictor': {'resource_plan': 'starter'}}}, 'scaling': {'backendDetails': {'predictor': {'maxReplicas': '3', 'minReplicas': '1', 'runningReplicas': 1}}, 'backend_details': {'predictor': {'max_replicas': '3', 'min_replicas': '1', 'running_replicas': 1}}}}\n",
      "scenarioId: learning-datalines\n",
      "configurationId: 8112fa53-1f28-4f02-aa11-4c55107e493f\n",
      "latestRunningConfigurationId: 8112fa53-1f28-4f02-aa11-4c55107e493f\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-06-24T06:48:08Z\n",
      "startTime: 2025-06-24T07:17:05Z\n",
      "completionTime: 2025-08-07T07:50:01Z\n",
      "configurationName: maintenance_sche_02\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d30e94d179443d52\n",
      "\n",
      "deployment 13:\n",
      "id: dcde9ede437debcd\n",
      "createdAt: 2025-06-23T08:02:51Z\n",
      "modifiedAt: 2025-06-24T06:48:08Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {'predictor': {'resourcePlan': 'starter'}}, 'backend_details': {'predictor': {'resource_plan': 'starter'}}}, 'scaling': {'backendDetails': {'predictor': {'maxReplicas': '3', 'minReplicas': '1', 'runningReplicas': 1}}, 'backend_details': {'predictor': {'max_replicas': '3', 'min_replicas': '1', 'running_replicas': 1}}}}\n",
      "scenarioId: learning-datalines\n",
      "configurationId: 8112fa53-1f28-4f02-aa11-4c55107e493f\n",
      "latestRunningConfigurationId: 8112fa53-1f28-4f02-aa11-4c55107e493f\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-06-23T08:06:13Z\n",
      "startTime: 2025-06-23T08:18:36Z\n",
      "completionTime: 2025-06-24T07:17:05Z\n",
      "configurationName: maintenance_sche_02\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dcde9ede437debcd\n",
      "\n",
      "deployment 14:\n",
      "id: d4ad94656a11e3dc\n",
      "createdAt: 2025-06-20T06:24:48Z\n",
      "modifiedAt: 2025-06-23T08:03:56Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {'predictor': {'resourcePlan': 'starter'}}, 'backend_details': {'predictor': {'resource_plan': 'starter'}}}, 'scaling': {'backendDetails': {'predictor': {'maxReplicas': '3', 'minReplicas': '1', 'runningReplicas': 1}}, 'backend_details': {'predictor': {'max_replicas': '3', 'min_replicas': '1', 'running_replicas': 1}}}}\n",
      "scenarioId: learning-datalines\n",
      "configurationId: 8112fa53-1f28-4f02-aa11-4c55107e493f\n",
      "latestRunningConfigurationId: 8112fa53-1f28-4f02-aa11-4c55107e493f\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-06-20T06:26:00Z\n",
      "startTime: 2025-06-20T06:36:02Z\n",
      "completionTime: 2025-06-23T08:11:22Z\n",
      "configurationName: maintenance_sche_02\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d4ad94656a11e3dc\n",
      "\n",
      "deployment 15:\n",
      "id: d76c23148b357a36\n",
      "createdAt: 2025-06-18T11:21:15Z\n",
      "modifiedAt: 2025-06-20T06:26:07Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {'predictor': {'resourcePlan': 'starter'}}, 'backend_details': {'predictor': {'resource_plan': 'starter'}}}, 'scaling': {'backendDetails': {'predictor': {'maxReplicas': '3', 'minReplicas': '1', 'runningReplicas': 1}}, 'backend_details': {'predictor': {'max_replicas': '3', 'min_replicas': '1', 'running_replicas': 1}}}}\n",
      "scenarioId: learning-datalines\n",
      "configurationId: 8112fa53-1f28-4f02-aa11-4c55107e493f\n",
      "latestRunningConfigurationId: 8112fa53-1f28-4f02-aa11-4c55107e493f\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-06-18T11:25:05Z\n",
      "startTime: 2025-06-18T11:37:21Z\n",
      "completionTime: 2025-06-20T06:36:02Z\n",
      "configurationName: maintenance_sche_02\n",
      "ttl: 3d\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d76c23148b357a36\n",
      "\n",
      "deployment 16:\n",
      "id: d34b36160b0e792d\n",
      "createdAt: 2025-06-12T14:33:29Z\n",
      "modifiedAt: 2025-08-07T07:41:33Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {'model': {'name': 'text-embedding-3-large', 'version': '1'}}, 'backend_details': {'model': {'name': 'text-embedding-3-large', 'version': '1'}}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: foundation-models\n",
      "configurationId: 5d6c2c8d-2013-48e1-9f70-82e967f256db\n",
      "latestRunningConfigurationId: 5d6c2c8d-2013-48e1-9f70-82e967f256db\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-06-12T14:35:15Z\n",
      "startTime: 2025-06-12T14:40:41Z\n",
      "completionTime: 2025-08-07T07:53:48Z\n",
      "configurationName: text-embedding-3-large_autogenerated\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d34b36160b0e792d\n",
      "\n",
      "deployment 17:\n",
      "id: da5de68204ff9585\n",
      "createdAt: 2025-06-12T11:05:10Z\n",
      "modifiedAt: 2025-08-07T07:41:36Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {'model': {'name': 'gemini-2.0-flash', 'version': '001'}}, 'backend_details': {'model': {'name': 'gemini-2.0-flash', 'version': '001'}}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: foundation-models\n",
      "configurationId: eee9981e-02ba-43f3-b051-07dbce2fdba0\n",
      "latestRunningConfigurationId: eee9981e-02ba-43f3-b051-07dbce2fdba0\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-06-12T11:06:35Z\n",
      "startTime: 2025-06-12T11:08:41Z\n",
      "completionTime: 2025-08-07T07:53:48Z\n",
      "configurationName: gemini-2.0-flash_autogenerated\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/da5de68204ff9585\n",
      "\n",
      "deployment 18:\n",
      "id: d8d6e1e95254702c\n",
      "createdAt: 2025-06-11T01:53:48Z\n",
      "modifiedAt: 2025-08-07T07:41:33Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {'model': {'name': 'gpt-4o-mini', 'version': 'latest'}}, 'backend_details': {'model': {'name': 'gpt-4o-mini', 'version': 'latest'}}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: foundation-models\n",
      "configurationId: f86b2ac2-89cf-4744-b103-b24c96590026\n",
      "latestRunningConfigurationId: f86b2ac2-89cf-4744-b103-b24c96590026\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-06-11T01:58:58Z\n",
      "startTime: 2025-06-11T02:12:33Z\n",
      "completionTime: 2025-08-07T07:53:48Z\n",
      "configurationName: Hello_worldof\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d8d6e1e95254702c\n",
      "\n",
      "deployment 19:\n",
      "id: d96d941f0655f2bf\n",
      "createdAt: 2025-06-11T01:53:42Z\n",
      "modifiedAt: 2025-08-07T07:41:36Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {'model': {'name': 'gpt-4o-mini', 'version': 'latest'}}, 'backend_details': {'model': {'name': 'gpt-4o-mini', 'version': 'latest'}}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: foundation-models\n",
      "configurationId: f86b2ac2-89cf-4744-b103-b24c96590026\n",
      "latestRunningConfigurationId: f86b2ac2-89cf-4744-b103-b24c96590026\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-06-11T01:58:58Z\n",
      "startTime: 2025-06-11T02:11:58Z\n",
      "completionTime: 2025-08-07T07:53:47Z\n",
      "configurationName: Hello_worldof\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d96d941f0655f2bf\n",
      "\n",
      "deployment 20:\n",
      "id: dc1263767fa81647\n",
      "createdAt: 2025-06-06T06:28:35Z\n",
      "modifiedAt: 2025-08-07T07:41:33Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {}, 'backend_details': {}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: orchestration\n",
      "configurationId: 8cecd483-0695-4fd6-87c5-b41359fa1627\n",
      "latestRunningConfigurationId: 8cecd483-0695-4fd6-87c5-b41359fa1627\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-06-06T06:30:23Z\n",
      "startTime: 2025-06-06T06:34:40Z\n",
      "completionTime: 2025-08-07T07:50:01Z\n",
      "configurationName: orchestration\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dc1263767fa81647\n",
      "\n",
      "deployment 21:\n",
      "id: de137c1711a5301b\n",
      "createdAt: 2025-05-23T09:06:31Z\n",
      "modifiedAt: 2025-08-07T07:41:33Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {}, 'backend_details': {}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: orchestration\n",
      "configurationId: 8cecd483-0695-4fd6-87c5-b41359fa1627\n",
      "latestRunningConfigurationId: 8cecd483-0695-4fd6-87c5-b41359fa1627\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-05-23T09:22:14Z\n",
      "startTime: 2025-05-23T09:23:43Z\n",
      "completionTime: 2025-08-07T07:50:02Z\n",
      "configurationName: orchestration\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/de137c1711a5301b\n",
      "\n",
      "deployment 22:\n",
      "id: dd6b70ffac147e90\n",
      "createdAt: 2025-05-22T18:19:01Z\n",
      "modifiedAt: 2025-08-07T07:41:33Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {}, 'backend_details': {}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: orchestration\n",
      "configurationId: 8cecd483-0695-4fd6-87c5-b41359fa1627\n",
      "latestRunningConfigurationId: 8cecd483-0695-4fd6-87c5-b41359fa1627\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-05-22T18:23:27Z\n",
      "startTime: 2025-05-22T18:57:10Z\n",
      "completionTime: 2025-08-07T07:50:01Z\n",
      "configurationName: orchestration\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dd6b70ffac147e90\n",
      "\n",
      "deployment 23:\n",
      "id: de1a39ead0caf762\n",
      "createdAt: 2025-05-22T07:34:53Z\n",
      "modifiedAt: 2025-08-07T07:41:33Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {}, 'backend_details': {}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: orchestration\n",
      "configurationId: 2b2f911e-59a0-46a2-9419-3fd9db293a25\n",
      "latestRunningConfigurationId: 2b2f911e-59a0-46a2-9419-3fd9db293a25\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-05-22T07:44:37Z\n",
      "startTime: 2025-05-22T07:50:36Z\n",
      "completionTime: 2025-08-07T07:50:02Z\n",
      "configurationName: defaultOrchestrationConfig\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/de1a39ead0caf762\n",
      "\n",
      "deployment 24:\n",
      "id: d85f19a9f4de343d\n",
      "createdAt: 2025-05-14T09:19:06Z\n",
      "modifiedAt: 2025-08-07T07:41:33Z\n",
      "status: STOPPED\n",
      "details: {'resources': {'backendDetails': {}, 'backend_details': {}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: orchestration\n",
      "configurationId: 2b2f911e-59a0-46a2-9419-3fd9db293a25\n",
      "latestRunningConfigurationId: 2b2f911e-59a0-46a2-9419-3fd9db293a25\n",
      "targetStatus: STOPPED\n",
      "submissionTime: 2025-05-14T09:23:26Z\n",
      "startTime: 2025-05-14T09:27:22Z\n",
      "completionTime: 2025-08-07T07:50:02Z\n",
      "configurationName: defaultOrchestrationConfig\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d85f19a9f4de343d\n"
     ]
    }
   ],
   "source": [
    "# Print result \n",
    "data = json.loads(api_response.text)\n",
    "# # Access the configuration list and print each key value details\n",
    "for i, deployment in enumerate(data[\"resources\"], start=1):\n",
    "    print(f\"\\ndeployment {i}:\")\n",
    "    for key, value in deployment.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "296d34b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 deployment(s):\n",
      "\n",
      "deployment #1:\n",
      "id: d07b68722141e03d\n",
      "createdAt: 2025-09-29T09:32:49Z\n",
      "modifiedAt: 2025-11-13T03:39:08Z\n",
      "status: RUNNING\n",
      "details: {'resources': {'backendDetails': {'model': {'name': 'gpt-4o', 'version': 'latest'}}, 'backend_details': {'model': {'name': 'gpt-4o', 'version': 'latest'}}}, 'scaling': {'backendDetails': {}, 'backend_details': {}}}\n",
      "scenarioId: foundation-models\n",
      "configurationId: ce1241ca-ae7b-43c5-9d80-e65535e60afa\n",
      "latestRunningConfigurationId: ce1241ca-ae7b-43c5-9d80-e65535e60afa\n",
      "lastOperation: CREATE\n",
      "targetStatus: RUNNING\n",
      "submissionTime: 2025-09-29T09:35:53Z\n",
      "startTime: 2025-09-29T09:38:07Z\n",
      "configurationName: cortex-1\n",
      "deploymentUrl: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d07b68722141e03d\n"
     ]
    }
   ],
   "source": [
    "# find all the deployment IDs of foundation-model configuration\n",
    "\n",
    "search_id = \"ce1241ca-ae7b-43c5-9d80-e65535e60afa\"  # The cofiguration ID of gpt-4o\n",
    "\n",
    "matches = [deployment for deployment in data.get(\"resources\", []) if deployment.get(\"configurationId\") == search_id]\n",
    "\n",
    "if matches:\n",
    "    print(f\"Found {len(matches)} deployment(s):\")\n",
    "    for idx, deploy in enumerate(matches, start=1):\n",
    "        print(f\"\\ndeployment #{idx}:\")\n",
    "        for key, value in deploy.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"No deployment found for configurationId:\", search_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9280a9b",
   "metadata": {},
   "source": [
    "### Querying the LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8aec2",
   "metadata": {},
   "source": [
    "Do the querying using generative-ai-hub-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ca0fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Generative AI refers to a subset of artificial intelligence that focuses on creating new content, such as images, text, music, or even videos, by learning patterns from existing data. Unlike traditional AI, which might focus on classification or prediction, generative AI is designed to produce novel outputs that resemble the data it was trained on.\n",
      "\n",
      "Key technologies and models in generative AI include:\n",
      "\n",
      "1. **Generative Adversarial Networks (GANs)**: These consist of two neural networks, a generator and a discriminator, that work in opposition. The generator creates new data instances, while the discriminator evaluates them for authenticity. Through this adversarial process, GANs can produce highly realistic images and other types of data.\n",
      "\n",
      "2. **Variational Autoencoders (VAEs)**: VAEs are a type of neural network that learns to encode input data into a compressed representation and then decode it back to the original form. They are used for generating new data by sampling from the latent space.\n",
      "\n",
      "3. **Transformers**: Models like GPT (Generative Pre-trained Transformer) are based on transformer architecture and are particularly effective in generating human-like text. They are trained on large datasets and can produce coherent and contextually relevant text.\n",
      "\n",
      "Generative AI has a wide range of applications, including\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_llm\n",
    " \n",
    "llm = init_llm(\n",
    "    model_name=MODEL_NAME,\n",
    "    temperature=0.0, \n",
    "    max_tokens=256)\n",
    "response = llm.invoke('What is generative AI?').content \n",
    "print('Response:', response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3cf487",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ef8d6",
   "metadata": {},
   "source": [
    "This example tasks the LLM with condensing and summarizing a given text. The text, clearly demarcated with triple backticks, is expected to be distilled into a concise summary of no more than 30 words. The focus is on extracting the most salient points and presenting them in a succinct manner, ensuring that the essence of the original content is retained without excessive verbosity. This format is designed to challenge the LLMs capability to discern key details and convey them efficiently. For this demo we have taken 2 pages from SAP annual report 2 on Independent Assurance Practitioners Report by KPMG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1d31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: KPMG conducted a limited assurance engagement on SAP SE's 2022 non-financial statement, finding no material misstatements and confirming compliance with relevant regulations and management interpretations.\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_llm\n",
    "\n",
    "prompt = \"\"\"Your task is to generate a short summary of a text.\n",
    "Summarize the text below, in at most 30 words. \n",
    "\n",
    "Review: Independent Assurance Practitioner's Report\n",
    "To the Supervisory Board of SAP SE, Walldorf\n",
    "We have performed a limited assurance engagement on the non-financial statement of SAP SE (further \"Company\" or \"SAP\") and on the non-financial statement of the parent company that is combined with it, which are published in the Management Report, (further \"combined non-financial statement\") for the period from January 1 to December 31, 2022.\n",
    "Responsibilities of Management\n",
    "Management of the company is responsible for the preparation of the combined non-financial statement in accordance with Sections 315c in conjunction with 289c to 289e HGB [\"Handelsgesetzbuch\": German Commercial Code] and Article 8 of REGULATION (EU) 2020/852 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of June 18, 2020 on establishing a framework to facilitate sustainable investment and amending Regulation (EU) 2019/2088 (hereinafter the \"EU Taxonomy Regulation\") and the Delegated Acts adopted thereunder, as well as for making their own interpretation of the wording and terms contained in the EU Taxonomy Regulation and the delegated acts adopted thereunder as set out in section \"Sustainable Finance: EU Taxonomy Disclosures\" of the combined non-financial statement.\n",
    "This responsibility includes the selection and application of appropriate non-financial reporting methods and making assumptions and estimates about individual non-financial disclosures of the group that are reasonable in the circumstances. Furthermore, management is responsible for such internal control as they consider necessary to enable the preparation of a combined non-financial statement that is free from material misstatement, whether due to fraud or error.\n",
    "The EU Taxonomy Regulation and the Delegated Acts issued thereunder contain wording and terms that are still subject to considerable interpretation uncertainties and for which clarifications have not yet been published in every case. Therefore, management has disclosed their interpretation of the EU Taxonomy Regulation and the Delegated Acts adopted thereunder in section \"Sustainable Finance: EU Taxonomy Disclosures\" of the combined non-financial statement. They are responsible for the defensibility of this interpretation. Due to the immanent risk that indeterminate legal terms may be interpreted differently, the legal conformity of the interpretation is subject to uncertainties.\n",
    "Independence and Quality Assurance of the Assurance Practitioner's firm\n",
    "We have complied with the independence and quality assurance requirements set out in the national legal provisions and professional pronouncements, in particular the Professional Code for German Public Auditors and Chartered Accountants (in Germany) and the quality assurance standard of the German Institute of Public Auditors (Institut der Wirtschaftsprufer, IDW) regarding quality assurance requirements in audit practice (IDW QS 1).\n",
    "Responsibility of the Assurance Practitioner\n",
    "Our responsibility is to express a conclusion with limited assurance on the combined non-financial statement based on our assurance engagement.\n",
    "We conducted our assurance engagement in accordance with International Standard on Assurance Engagements (ISAE) 3000 (Revised): \"Assurance Engagements other than Audits or Reviews of\n",
    "41/335\n",
    "  SAP Integrated Report 2022\n",
    " To Our Stakeholders\n",
    "    Combined Group Consolidated Financial Further Information on Management Report Statements IFRS Sustainability\n",
    "  Additional Information\n",
    "  Historical Financial Information\" issued by the IAASB. This standard requires that we plan and perform the assurance engagement to obtain limited assurance about whether any matters have come to our attention that cause us to believe that the company's non-financial statement, is not prepared, in all material respects, in accordance with Sections 315c in conjunction with 289c to 289e HGB and the EU Taxonomy Regulation and the Delegated Acts issued thereunder as well as the interpretation by management disclosed in section \"Sustainable Finance: EU Taxonomy Disclosures\" of the combined non-financial statement. We do not, however, issue a separate conclusion for each disclosure.\n",
    "In a limited assurance engagement, the procedures performed are less extensive than in a reasonable assurance engagement, and accordingly, a substantially lower level of assurance is obtained. The selection of the assurance procedures is subject to the professional judgment of the assurance practitioner.\n",
    "In the course of our assurance engagement we have, among other things, performed the following assurance procedures and other activities:\n",
    " Interviewing employees responsible for the materiality analysis at group level in order to obtain an understanding on the approach for identifying key issues and related reporting limits of SAP,\n",
    " Carrying out a risk assessment, inclusive of media analysis, on relevant information on sustainability performance of SAP in the reporting period,\n",
    " Assessing the design and implementation of systems and processes for identifying, handling, and monitoring information on environmental, employee and social matters, human rights and combating corruption and bribery, including the consolidation of data,\n",
    " Interviewing staff on group level, who are responsible for the disclosures on concepts, due diligence processes, results and risks, the performance of internal control activities and the consolidation of the disclosures,\n",
    " Inspecting selected internal and external documents,\n",
    " Analytically assessing the data and trends of the quantitative information, which is reported on group level of all locations,\n",
    " Evaluating the local data collection, validation, and reporting processes as well as the reliability of the reported data by means of a sampling survey at two locations,\n",
    " Interviewing of responsible staff on group level to obtain an understanding of the approach to identify relevant economic activities in accordance with the EU taxonomy,\n",
    " Evaluating the design and implementation of systems and procedures for identifying, processing, and monitoring information on turnover, capital expenditures and operating expenditures for the taxonomy-relevant economic activities for the first two environmental objectives climate change mitigation and climate change adaptation,\n",
    " Evaluating the data collection, validation, and reporting processes, as well as the reliability of the reported data for the taxonomy-aligned economic activities in conjunction with the assessment of the technical evaluation criteria for the substantial contribution, the fulfilment of the DNSH-criteria and the documentation of the minimum safeguard,\n",
    " Assessment of the overall presentation of the disclosures.\n",
    "In determining the disclosures in accordance with Article 8 of the EU Taxonomy Regulation, management is required to interpret undefined legal terms. Due to the immanent risk that undefined legal terms may be interpreted differently, the legal conformity of their interpretation and, accordingly, our assurance engagement thereon are subject to uncertainties.\n",
    "Assurance Opinion\n",
    "Based on the assurance procedures performed and the evidence obtained, nothing has come to our attention that causes us to believe that the combined non-financial statement of SAP SE, Walldorf for\n",
    "42/335\n",
    "\n",
    " SAP Integrated Report 2022\n",
    "To Our Stakeholders\n",
    "Combined Group Consolidated Financial Further Information on Management Report Statements IFRS Sustainability\n",
    "Additional Information\n",
    "the period from January 1 to December 31, 2022 has not been prepared, in all material respects, in accordance with Sections 315c in conjunction with 289c to 289e HGB and the EU Taxonomy Regulation and the Delegated Acts issued thereunder as well as the interpretation by management as disclosed in section \"Sustainable Finance: EU Taxonomy Disclosures\" of the combined non-financial statement.\n",
    "Restriction of Use\n",
    "This assurance report is solely addressed to SAP SE, Walldorf.\n",
    "Our assignment for SAP SE, Walldorf and professional liability is governed by the General Engagement Terms for Wirtschaftsprufer (German Public Auditors) and Wirtschaftsprufungs- gesellschaften (German Public Audit Firms) (Allgemeine Auftragsbedingungen fur Wirtschaftsprufer und Wirtschaftsprufungsgesellschaften) in the version dated January 1, 2017 (https://www.kpmg.de/bescheinigungen/lib/aab_english.pdf). By reading and using the information contained in this assurance report, each recipient confirms having taken note of provisions of the General Engagement Terms (including the limitation of our liability for negligence to EUR 4 million as stipulated in No. 9) and accepts the validity of the attached General Engagement Terms with respect to us.\n",
    "Mannheim, den 22. Februar 2023\n",
    "KPMG AG Wirtschaftsprufungsgesellschaft\n",
    "Beyer\n",
    "Wirtschaftsprufer [German Public Auditor]\n",
    "Wiegand Wirtschaftsprufer [German Public Auditor]\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "llm = init_llm(\n",
    "    model_name=MODEL_NAME,\n",
    "    temperature=0.0, \n",
    "    max_tokens=256)\n",
    "response = llm.invoke(prompt).content \n",
    "print('Response:', response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e54b5",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90706ec",
   "metadata": {},
   "source": [
    "This example instructs the LLM to formulate a concise response to a specific question, with the context provided for reference. The LLMs answer should be encapsulated within triple backticks, ensuring a clear distinction between the question and the response. If the LLM is uncertain about the correct answer based on the provided context, it is instructed to reply with Unsure about answer, offering a clear acknowledgment of uncertainty rather than providing potentially inaccurate information. Where we took a part of memo by Christian Klein from SAP Annual report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2878a2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: SAP is performing well, with strong financial results in 2022. Cloud revenue and backlog both increased by 24%, and total revenue grew by 5%. However, the share price decreased by 22.8%, and the Customer Net Promoter Score and Employee Engagement Index saw declines. Despite these challenges, SAP remains committed to optimizing its business and expanding its position as a leading enterprise application company.\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_llm\n",
    "\n",
    "prompt = \"\"\"Answer the question  based on the context below. Keep the answer short and concise. \n",
    "Respond \"Unsure about answer\" if not sure about the answer.\n",
    "Context: Dear Fellow Shareholders,\n",
    "It's hard to summarize the year 2022 in a few words  as the pandemic continued, the world also faced new and unexpected challenges, such as the terrible war in Ukraine, that hugely impacted all of our lives. We have faced conflicts and geopolitical tensions, climate change, the energy crisis, inflation, and volatile markets. Yet, once again, we have seen solidarity in times of crisis  people coming together to provide support to those in need when it mattered most. Despite the ongoing uncertainties in the world, SAP has remained in a strong position.\n",
    "2022 marked the 50th anniversary of SAP, which we celebrated together with our customers, partners, and colleagues across the world. Five decades ago, our founders set out to redefine business software and in doing so, forever changed the way the world runs. Their innovative thinking, pioneering spirit, and drive laid the foundation for the rise of SAP  and they are still the basis for our success today, as we are carrying their legacy forward to drive positive change for our planet and its people  something that has never been more relevant or important than today.\n",
    "Our hearts remain with the people impacted by the war in Ukraine. As announced, SAP has stopped all sales in Russia and Belarus, and we are in the process of a total withdrawal from these markets. For 2023, while business wind-down continues, our focus is on further reducing the remaining SAP footprint in Russia. We hope for the swift restoration of peace and will continue to help those affected by this war.\n",
    "7/335\n",
    "  SAP Integrated Report 2022\n",
    " To Our Stakeholders\n",
    "    Combined Group Consolidated Financial Further Information on Management Report Statements IFRS Sustainability\n",
    "  Additional Information\n",
    "  The last year was a stark reminder that no one business, government, or society can tackle the greatest challenges of our time alone. For that, a change is needed, and technology plays a key role in finding solutions to our global challenges.\n",
    "Over two years ago, we embarked on our transformation journey to move SAP towards a cloud company. This, together with our vision to enable every organization and every industry to become a network of intelligent, sustainable enterprises, is perfectly aligned with the challenges our customers face. From increasing speed and agility, building transparent and resilient supply chains, and recording, reporting, and acting on sustainability, our solutions provide the value our customers need:\n",
    " With RISE with SAP, we enable organizations to become agile, intelligent enterprises.\n",
    " With the SAP Business Network, we connect millions of companies, allowing organizations to\n",
    "benefit from connected networks.\n",
    " With our SAP sustainability solutions, we enable organizations to truly operate sustainably.\n",
    "Looking at our financial numbers, we met all of our outlook metrics in 2022. Our strong full-year 2022 results at a glance:\n",
    " Cloud revenue continued to be our main growth driver, increasing by 24%1.\n",
    " Current cloud backlog increased by 24%1.\n",
    " Total revenue grew 5%1.\n",
    " IFRS Operating profit was flat, while non-IFRS operating profit decreased by 7%1.\n",
    " Free cash flow was 4.35 billion.\n",
    "2022 was a volatile year on the market, with technology stocks particularly hard hit. Our shares were not immune from this overall trend. Our share price decreased 22.8% in 2022, below the DAX, which lost 12.4%, but better than the NASDAQ 100, which decreased 33% over the course of the year. We want our shareholders to participate in our success. Therefore, we have proposed an annual dividend of 2.05 per share2,  an increase of approximately 5% over the prior year's regular dividend.\n",
    "Customer Net Promoter Score (NPS) decreased 7 points year over year to 3 in 2022, hitting the lower end of the revised outlook range. SAP's Employee Engagement Index decreased 3 percentage points to 80%, a continued high level of engagement at the low end of the revised outlook range. The software as a service-industry scores overall have declined over the past few years of the pandemic. SAP continues to get feedback about needed improvements around pricing increases, licensing structure, product-related topics, support, service and stability of account team relationships. This type of transparent feedback and accountability helps provide us with the information to better focus investments and further improve our customer relationships. SAP's retention rate was 92.3% (2021: 92.8%). Further, the proportion of women in management increased to 29.4% (2021: 28.3%) and we also reached 35% of women in the workforce. Net carbon emissions continued to decrease, at 85 kilotons in 2022, down 25 kt year over year.\n",
    "In addition to driving our ESG goals internally, we also take our wider social and environmental responsibility very seriously:\n",
    " In total, SAP donated more than 4.2 million to support Ukraine in cooperation with organizations such as UNICEF, UNHCR, and the German Red Cross. This includes our employee donation campaign which became SAP's largest employee donation campaign to date.\n",
    " SAP extended its partnership with UNICEF through Generation Unlimited (GenU), focusing on employability. The partnership also supports SAP Educate to Employ, a new program educating\n",
    "1 At constant currencies\n",
    "2 Pending approval of Annual General Meeting of Shareholders\n",
    " 8/335\n",
    "\n",
    "  SAP Integrated Report 2022\n",
    " To Our Stakeholders\n",
    "    Combined Group Consolidated Financial Further Information on Management Report Statements IFRS Sustainability\n",
    "  Additional Information\n",
    "  youth in need on soft skills, foundational knowledge, and SAP skills to enable a pathway to a successful career in the SAP ecosystem.\n",
    " Together with partners, we launched the TRANSFORM Support Hub offering virtual pro bono consulting opportunities worldwide, connecting SAP employees to social enterprises.\n",
    "Net-net: We believe that together with our customers, colleagues, and partners around the world, we can turn the world's greatest challenges into opportunities for a prosperous and greener future. 2022 was one of the most important years in our history. As we head into 2023, we are committed to further optimizing and improving our business. We are deepening our focus on delivering lifetime value to current and new customers in the cloud and on high-growth opportunities where SAP can lead. Across SAP, we are laying the foundation for SAP's ongoing success, expanding our position as the #1 Enterprise Application company on the planet, powered by our leading platform. \n",
    "Finally, I want to express my deepest thanks for your continuous trust in SAP. I certainly look back on 2022 with pride and gratitude for the many ways SAP's teams around the world are making a difference. I'm very much looking forward to 2023, and the great achievements our over 100,000 colleagues will continue to deliver as we pursue our vision to enable every organization and every industry to become a network of intelligent, sustainable enterprises.\n",
    "Sincerely,\n",
    "Christian Klein CEO, SAP SE\n",
    "9/335\n",
    "\n",
    "  \n",
    " SAP Integrated Report 2022\n",
    "To Our Stakeholders\n",
    "Combined Group Consolidated Financial Further Information on Management Report Statements IFRS Sustainability\n",
    "Additional Information\n",
    "SAP Executive Board\n",
    "Question: How is SAP performing?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "llm = init_llm(\n",
    "    model_name=MODEL_NAME,\n",
    "    temperature=0.0, \n",
    "    max_tokens=256)\n",
    "response = llm.invoke(prompt).content \n",
    "print('Response:', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abce273",
   "metadata": {},
   "source": [
    "### Text Classification - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c41bf",
   "metadata": {},
   "source": [
    "This example directs an LLM to perform sentiment analysis on a provided product review. The LLM is instructed to assess the sentiment of the review text and respond with a single word, either positive or negative. The review text is clearly delineated using triple backticks, ensuring clarity about which portion of the text needs to be analyzed. This format aims to extract concise and direct sentiment evaluations without any ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48d9b98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Positive\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_llm\n",
    "\n",
    "prompt = \"\"\"What is the sentiment of the following product review, \n",
    "which is delimited with triple single quotes?\n",
    "\n",
    "Give your answer as a single word, either \"positive\" or \"negative\".\n",
    "\n",
    "Review text: '''SAP has Best work Environment and Best ERP product'''\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "llm = init_llm(\n",
    "    model_name=MODEL_NAME,\n",
    "    temperature=0.0, \n",
    "    max_tokens=256)\n",
    "response = llm.invoke(prompt).content \n",
    "print('Response:', response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116f142",
   "metadata": {},
   "source": [
    "### Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968555d",
   "metadata": {},
   "source": [
    "Here in this demo we are going to try out show text expansion qualities of AI core, where we wrote a few sentences about AI core and asked the LLM model to write a 500 word blog post or paragraph based on the context it understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f3dca621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: **Unlocking the Future of AI with SAP AI Core: A Comprehensive Platform for Building, Training, and Deploying AI Applications**\n",
      "\n",
      "In the rapidly evolving landscape of artificial intelligence, businesses are constantly seeking robust platforms that can streamline the development, training, and deployment of AI applications. SAP AI Core emerges as a powerful solution, offering a comprehensive suite of tools designed to meet these needs. This platform not only facilitates the creation of AI applications but also serves as a model and dataset artifactory, making it an indispensable asset for organizations aiming to harness the full potential of AI.\n",
      "\n",
      "**Building AI Applications with SAP AI Core**\n",
      "\n",
      "At the heart of SAP AI Core is its capability to support the entire lifecycle of AI application development. From ideation to deployment, the platform provides a seamless environment where developers can build sophisticated AI models. The platform is designed to be user-friendly, allowing both seasoned data scientists and newcomers to AI to create applications with ease. By offering a range of pre-built templates and integration options, SAP AI Core accelerates the development process, enabling businesses to bring their AI solutions to market faster.\n",
      "\n",
      "**Training AI Models Efficiently**\n",
      "\n",
      "Training AI models is a resource-intensive process that requires significant computational power and data management capabilities. SAP AI Core addresses these challenges by providing a scalable infrastructure that\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_llm\n",
    "\n",
    "prompt = \"\"\"Expand it into a 500 word blog post\n",
    "SAP AI core is a platform for building AI applications. which can be used to train and deploy AI applications. as well as act as a model and dataset artifactory.\"\"\"\n",
    "\n",
    "\n",
    "llm = init_llm(\n",
    "    model_name=MODEL_NAME,\n",
    "    temperature=0.0, \n",
    "    max_tokens=256)\n",
    "response = llm.invoke(prompt).content \n",
    "print('Response:', response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad26655",
   "metadata": {},
   "source": [
    "### Tone adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79baf61a",
   "metadata": {},
   "source": [
    "Here in tone Adjustment we are trying to showcase use the LLMs capabilities to change/modify the tone of a text written by a new employee at SAP to proper professional tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e43474c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Introducing SAP Blue, an innovative offering from SAP designed to cater to the dynamic gig economy. Originally launched in 2016, SAP Blue has been revitalized and reintroduced in 2023 to better serve the evolving needs of modern businesses and independent professionals.\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_llm\n",
    "\n",
    "prompt = \"\"\"Translate the following input to a Corporate language\n",
    "Sap Blue a new product from SAP. that is a gig based product. launched in 2016 relaunched in 2023.\"\"\"\n",
    "\n",
    "llm = init_llm(\n",
    "    model_name=MODEL_NAME,\n",
    "    temperature=0.0, \n",
    "    max_tokens=256)\n",
    "response = llm.invoke(prompt).content \n",
    "print('Response:', response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d319e6",
   "metadata": {},
   "source": [
    "### Spell-check / Grammar-check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7e4da",
   "metadata": {},
   "source": [
    "Here we picked a paragraph on SAP and made a few spelling and grammatical errors. Now we will be asking the LLM to fix those errors by proofreading the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "af98d296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: SAP SE is a German multinational software company based in Walldorf, Baden-Wrttemberg. It develops enterprise software to manage business operations and customer relations. The company is the world's leading enterprise resource planning software vendor.\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_llm\n",
    "\n",
    "prompt = \"\"\"proofread and correct this review: SAP SE is a German multinationl software company based in Walldorf, Baden-Wurttemberg. It develops enterprise software to manage business operations and customer relations. The company is the world leading enterprise resource planing software vendor.\"\"\"\n",
    "\n",
    "llm = init_llm(\n",
    "    model_name=MODEL_NAME,\n",
    "    temperature=0.0, \n",
    "    max_tokens=256)\n",
    "response = llm.invoke(prompt).content \n",
    "print('Response:', response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6bbe9",
   "metadata": {},
   "source": [
    "### Doing multiple tasks at once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba887a64",
   "metadata": {},
   "source": [
    "We picked up a random review from trust pilot on SAP ERP and want the LLM to perform multiple tasks at once which include sentiment analysis, checking if the user is angry with the product, which product/item they are talking about and which brand does it belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a5a9b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: ```json\n",
      "{\n",
      "  \"Sentiment\": \"positive\",\n",
      "  \"Anger\": false,\n",
      "  \"Item\": \"ERP software\",\n",
      "  \"Brand\": \"SAP\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_llm\n",
    "\n",
    "prompt = \"\"\"Identify the following items from the review text: \n",
    "- Sentiment (positive or negative)\n",
    "- Is the reviewer expressing anger? (true or false)\n",
    "- Item purchased by reviewer\n",
    "- Company that made the item\n",
    "\n",
    "The review is delimited with triple backticks.\n",
    "Format your response as a JSON object with \"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys.\n",
    "If the information isn't present, use \"unknown\" as the value.\n",
    "Make your response as short as possible. Format the Anger value as a boolean.\n",
    "\n",
    "Review text: '''A true ERP software available in the market which captures 60% of market share and known as a ERP leader. The best part about the product that it can be used cloud based and it can be integrated with several modules which are equally relevant as a department. I have been a part of SAP from past 12+ years and I am extremely happy of using and referring this product to others as well. This isn't only beneficial for companies but can also make careers for humans as well. Now a days cloud based functionality and integration with API tools are the best part in it. Easy to customize according to the requirement of a client. It has several features and capabilities 1. Cost efficient, 2. Advance data management, 3. Saves time, 4. Increase productivity, 5. Real time data saving to server's, 6. Avoid duplication and ensures transparency. Best product available in the market.''' \"\"\"\n",
    "\n",
    "llm = init_llm(\n",
    "    model_name=MODEL_NAME,\n",
    "    temperature=0.0, \n",
    "    max_tokens=256)\n",
    "response = llm.invoke(prompt).content \n",
    "print('Response:', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79015eec",
   "metadata": {},
   "source": [
    "### Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33fff5e",
   "metadata": {},
   "source": [
    "The following example demonstrates a few-shot learning approach in prompt engineering, where the model is provided with a couple of examples to understand the desired task and format. Instead of explicitly stating the task, the LLM is given a context in which it should operate.\n",
    "\n",
    "* Contextual Setup: The Child and Grandparent dialog sets up a context. The model is implicitly being taught that it should generate responses in the style of a wise grandparent answering a childs questions.\n",
    "* Example Provided: The first complete interaction (about patience) serves as a shot or example, guiding the model on how it should structure its response.\n",
    "* Task Indication: The second interaction (about unity) is incomplete, indicating the task the model needs to perform. The goal is to get the model to continue the pattern and provide a similarly styled, profound answer to the childs new question.\n",
    "\n",
    "The models generated completion is then printed, providing insight into its understanding and continuation of the provided examples.\n",
    "\n",
    "In summary, by using a few-shot learning approach, the model is guided to understand and emulate the style of the conversation without explicitly being told the exact format or context. This method leverages the models ability to generalize from few examples and produce consistent and contextually relevant outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fec2f567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Just as a single drop of rain joins countless others to form a mighty ocean,  \n",
      "and a lone star finds its place among the constellations,  \n",
      "so too do we find strength and beauty in coming together.  \n",
      "Each of us, like a thread in a vast quilt, contributes to a greater whole,  \n",
      "creating warmth and harmony that none could achieve alone.\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_llm\n",
    "\n",
    "prompt = \"\"\"Your task is to answer in a consistent style.\n",
    "\n",
    "Child: Teach me about patience.\n",
    "\n",
    "Grandparent: \n",
    "The river that carves the deepest valley flows from a modest spring;\n",
    "the grandest symphony originates from a single note; \n",
    "the most intricate tapestry begins with a solitary thread.\n",
    "\n",
    "Child: Teach me about unity.\n",
    "\n",
    "Grandparent:\"\"\"\n",
    "\n",
    "llm = init_llm(\n",
    "    model_name=MODEL_NAME,\n",
    "    temperature=0.0, \n",
    "    max_tokens=256)\n",
    "response = llm.invoke(prompt).content \n",
    "print('Response:', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85852ad4",
   "metadata": {},
   "source": [
    "### Zero-Shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfec3d8",
   "metadata": {},
   "source": [
    "The following example showcases an advanced application of prompt engineering that encapsulates a multi-step task for the LLM.\n",
    "\n",
    "* Role Emulation: The initial statement sets the stage by instructing the LLM to assume the role of a customer service AI assistant. This prepares the model to respond in a specific, customer-service oriented manner.\n",
    "\n",
    "* Multi-step Instruction: The prompt is divided into two distinct steps, each guiding the model to perform a specific action:\n",
    "\n",
    "    * Step 1: Sentiment Analysis - The model is directed to discern the sentiment of a provided customer review. This sentiment could be positive, negative, or neutral.\n",
    "    \n",
    "    * Step 2: Contextual Reply - Based on the extracted sentiment, the model must generate an appropriate email reply. The instructions are clear:\n",
    "        * Positive or Neutral Sentiment: Express gratitude.\n",
    "        * Negative Sentiment: Offer an apology and provide an avenue for further assistance. The model is also guided to incorporate details from the review to ensure the response feels tailored and specific to the customers concerns.\n",
    "* Tone and Format: The instructions emphasize writing in a concise and professional tone. Moreover, the model is guided to sign off the email as AI customer agent, reinforcing the context and ensuring the generated response follows a proper email format.\n",
    "\n",
    "* Multi-step Continuation: This prompt is an excellent example of multi-step continuation. The LLM first determines the sentiment of the review and then uses that sentiment to guide its next action, which is generating a contextually appropriate email reply.\n",
    "\n",
    "In summary, from a prompt engineering standpoint, this example effectively leverages a structured, multi-step instruction set to guide the LLM through a complex task. The clarity and specificity of the instructions, combined with the defined role and context, aim to elicit a precise and contextually relevant response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c25c794a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: **Step 1: Sentiment Analysis**\n",
      "\n",
      "The sentiment of the customer review is negative. The customer expresses dissatisfaction with the price increase, concerns about the product's quality compared to previous editions, and issues with the motor after a year of use. They also mention the warranty had expired, requiring them to purchase another unit.\n",
      "\n",
      "**Step 2: Email Reply**\n",
      "\n",
      "Subject: Thank You for Your Feedback\n",
      "\n",
      "Dear [Customer's Name],\n",
      "\n",
      "Thank you for taking the time to share your experience with us regarding the 17-piece system. We sincerely apologize for any inconvenience caused by the price changes and the issues you encountered with the product's quality and motor performance.\n",
      "\n",
      "We understand how important it is to have reliable and high-quality products, and we are committed to improving our offerings. Your feedback is invaluable in helping us achieve this goal. If you have any further concerns or need assistance, please feel free to reach out to our customer service team, who will be more than happy to assist you.\n",
      "\n",
      "Thank you once again for your feedback and for being a valued customer.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "AI Customer Agent\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_llm\n",
    "\n",
    "prompt = \"\"\"\n",
    "\n",
    "You are a customer service AI assistant. \n",
    "Given the customer email, perform the following steps: \n",
    "\n",
    "Step 1: \n",
    "Extract the sentiment of the customer review as positive, negative and neutral. \n",
    "\n",
    "Step 2: \n",
    "Now, Your task is to send an email reply to a valued customer.\n",
    "Generate a reply to thank the customer for their review.\n",
    "If the sentiment is positive or neutral, thank them for their review.\n",
    "If the sentiment is negative, apologize and suggest that they can reach out to customer service. \n",
    "Make sure to use specific details from the review. Write in a concise and professional tone.\n",
    "Sign the email as `AI customer agent`.\n",
    "\n",
    "Customer review: \n",
    "So, they still had the 17 piece system on seasonal sale for around $49 in the month of November, about half off, but for some reason (call it price gouging) around the second week of December the prices all went up to about anywhere from between $70-$89 for the same system. And the 11 piece system went up around $10 or so in price also from the earlier sale price of $29. So it looks okay, but if you look at the base, the part where the blade locks into place doesn't look as good as in previous editions from a few years ago, but I plan to be very gentle with it (example, I crush very hard items like beans, ice, rice, etc. in the blender first then pulverize them in the serving size I want in the blender then switch to the whipping blade for a finer flour, and use the cross cutting blade first when making smoothies, then use the flat blade if I need them finer/less pulpy). Special tip when making smoothies, finely cut and freeze the fruits and vegetables (if using spinach-lightly stew soften the spinach then freeze until ready for use-and if making sorbet, use a small to medium sized food processor) that you plan to use that way you can avoid adding so much ice if at all-when making your smoothie. After about a year, the motor was making a funny noise. I called customer service, but the warranty expired already, so I had to buy another one. FYI: The overall quality has gone done in these types of products, so they are kind of counting on brand recognition and consumer loyalty to maintain sales. Got it in about two days.\n",
    "\"\"\"\n",
    "\n",
    "llm = init_llm(\n",
    "    model_name=MODEL_NAME,\n",
    "    temperature=0.0, \n",
    "    max_tokens=256)\n",
    "response = llm.invoke(prompt).content \n",
    "print('Response:', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e614e0b",
   "metadata": {},
   "source": [
    "[*Tutorial02 - Retrieval Augmented Generation using generative-ai-hub-sdk and HANA vector search*](https://developers.sap.com/tutorials/ai-core-genai-hana-vector.html)\n",
    "## Retrieval Augmented Generation using generative-ai-hub-sdk and HANA vector search\n",
    "Using HANA vector store to store vector embeddings and using them in Retrieval Augmented Generation.\n",
    " \n",
    "* How to create a table and store embeddings in HANA Vector Store.\n",
    "* How to use the embeddings in Retrieval Augmented Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efae350",
   "metadata": {},
   "source": [
    "[*Tutorial03 - Using foundational models on SAP AI Core*](https://developers.sap.com/tutorials/ai-core-consumption-llm.html)\n",
    "## Using foundational models on SAP AI Core\n",
    "In this tutorial we are going to learn on how to consume LLM on AI core deployed on SAP AI core.\n",
    "* How to inference foundational models on AI core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "060e9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Library\n",
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "ai_core_client = AICoreV2Client(\n",
    "    base_url = \"https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com\" + \"/v2\", # The present SAP AI Core API version is 2\n",
    "    auth_url=  \"https://rb-btphub-taf-d.authentication.eu10.hana.ondemand.com\" + \"/oauth/token\", # Suffix to add\n",
    "    client_id = \"sb-68b507a6-6c60-4dfe-823a-9b7d4ba2f409!b550689|aicore!b540\",\n",
    "    resource_group = \"default\",\n",
    "    client_secret = \"1298f815-2d07-4e28-9dc6-5ff4cfc82c00$VB5IA6pyBIR6ca6kY-c_lJ98d1oGIIXCTpThiA0WE08=\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "071cde21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ID: dac8d37a6fd75edc\n",
      "Name: anthropic claude-3.5 sonnet_ssconfig\n",
      "Model: anthropic--claude-3.5-sonnet\n",
      "Version: latest\n",
      "Status: RUNNING\n",
      "Created At: 2025-11-13 07:35:47+00:00\n",
      "Deployment URL: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dac8d37a6fd75edc\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model name you want to search for\n",
    "target_model = \"anthropic--claude-3.5-sonnet\"  # Change this to your desired model name\n",
    "\n",
    "response = ai_core_client.deployment.query()\n",
    "\n",
    "for deployment in response.resources:\n",
    "    backend_details = deployment.details.get('resources', {}).get('backend_details', {})\n",
    "    model_info = backend_details.get('model')\n",
    "\n",
    "    # Check if 'model' exists and matches the target model name\n",
    "    if model_info and model_info.get('name') == target_model:\n",
    "        print(f\"\"\"\n",
    "ID: {deployment.id}\n",
    "Name: {deployment.configuration_name}\n",
    "Model: {model_info.get('name')}\n",
    "Version: {model_info.get('version')}\n",
    "Status: {deployment.status.value}\n",
    "Created At: {deployment.created_at}\n",
    "Deployment URL: {deployment.deployment_url}\n",
    "----------------------------------------\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a85aba0",
   "metadata": {},
   "source": [
    "### anthropic--claude-3.5-sonnet\n",
    "In this example we will see how to consume this generative AI model using Generative AI Hub SDK.\n",
    "Before you use these models, please ensure that the deployment has already been created. You can create the deployment either through generative-ai-hub-sdk or AI Launchpad.\n",
    "For inferencing the corresponding model through Generative AI Hub SDK, execute the following python command -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b8628ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 'hello world' program demonstrates the basic syntax of a programming language by outputting a simple greeting.\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.native.amazon.clients import Session\n",
    "bedrock = Session().client(model_name=\"anthropic--claude-3.5-sonnet\")\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "response = bedrock.converse(\n",
    "    messages=conversation,\n",
    "    inferenceConfig={\"maxTokens\": 512, \"temperature\": 0.5, \"topP\": 0.9},\n",
    ")\n",
    "print(response['output']['message']['content'][0]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce34e02",
   "metadata": {},
   "source": [
    "[*Tutorial04 - Using Multimodal inputs with GPT4o for Image Recognition on SAP AI Core*](https://developers.sap.com/tutorials/ai-core-gpt4o-consumption.html)\n",
    "## Using Multimodal inputs with GPT4o for Image Recognition on SAP AI Core\n",
    "Multimodality refers to the ability of a model to process and interpret different types of inputs, such as text, images, audio, or video. In the context of GPT-4o on SAP AI Core, multimodal input allows the model to understand and generate responses that incorporate both text and visual data. This enhances the models ability to perform complex tasks, such as scene detection, object recognition, and image analysis, by combining the strengths of both language processing and image recognition.In this tutorial, we will demonstrate these capabilities with the help of GPT-4o, with a sample input and output, which can be replicated in future for various use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec630573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource group is set to: default\n"
     ]
    }
   ],
   "source": [
    "from config import init_env\n",
    "from config import variables\n",
    "import importlib\n",
    "variables = importlib.reload(variables)\n",
    "\n",
    "# TODO: You need to specify which model you want to use. In this case we are directing our prompt\n",
    "# to the openAI API directly so you need to pick one of the GPT models. Make sure the model is actually deployed\n",
    "# in genAI Hub. You might also want to chose a model that can also process images here already. \n",
    "# E.g. 'gpt-4.1-mini'\n",
    "MODEL_NAME = 'gpt-4o'\n",
    "\n",
    "# Do not modify the `assert` line below\n",
    "assert MODEL_NAME!='', \"\"\"You should change the variable `MODEL_NAME` with the name of your deployed model (like 'gpt-4o-mini') first!\"\"\"\n",
    "\n",
    "init_env.set_environment_variables()\n",
    "# Do not modify the `assert` line below \n",
    "assert variables.RESOURCE_GROUP!='', \"\"\"You should change the value assigned to the `RESOURCE_GROUP` in the `variables.py` file to your own resource group first!\"\"\"\n",
    "\n",
    "print(f\"Resource group is set to: {variables.RESOURCE_GROUP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eedd4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.native.openai import chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe75beb",
   "metadata": {},
   "source": [
    "### Scene Detection\n",
    "In this step, we demonstrate how to use GPT-4o to describe a scene depicted in an image. By providing both text and an image URL as input, the model is able to generate a descriptive response that captures the key elements of the scene. This capability is particularly useful for applications like automated content tagging, visual storytelling, or enhancing user experience in multimedia platforms and more.\n",
    "\n",
    "Follow the further steps to replicate scene detection using GPT-4o.\n",
    "\n",
    "To utilize the GPT-4o model, which supports both text and image inputs, use the code below. This example demonstrates how to create a prompt with an image URL and a text query, enabling the model to process and provide a response based on both visual and textual information.\n",
    "\n",
    "Note: You can replace the image URL with any image of your choice and modify the text prompt to ask the model any question about that image based on your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d06c3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "def encode_image_from_url(image_url):\n",
    "    \"\"\"Download and encode image to base64 format from URL.\"\"\"\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        return base64.b64encode(response.content).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download image. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ccfc9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_prompt(image_url, text_prompt):\n",
    "    \"\"\"Create a prompt message for the model with the image data.\"\"\"\n",
    "    # Encode image URL to base64 format\n",
    "    image_base64 = encode_image_from_url(image_url)\n",
    "    \n",
    "    # Create messages including both text and image input\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": text_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url  # Use the direct image URL\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "85d7b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_model(model_name, messages):\n",
    "    \"\"\"Send messages to the model and return the response.\"\"\"\n",
    "    kwargs = dict(model_name=model_name, messages=messages)\n",
    "    response = chat.completions.create(**kwargs)\n",
    "    return response.to_dict()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a7b5f",
   "metadata": {},
   "source": [
    "By following this example, you can easily integrate image-based inputs with the GPT-4o model and leverage its ability to understand and generate responses based on both visual and text content. For additional guidance, refer to the screenshot below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/sceneDetection.jpg\" width=\"30%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "10608845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a blue water bottle and black headphones on a desk beside an electrical outlet in a room with maroon and beige walls.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_url = \"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/sceneDetection.jpg\"\n",
    "text_prompt = \"Describe the image in one line.\"  # Prompt asking for the description\n",
    "model_name = \"gpt-4o\"  # Replace with the model that supports image input\n",
    "\n",
    "# Create prompt with image and text\n",
    "messages = create_image_prompt(image_url, text_prompt)\n",
    "\n",
    "# Get response from model\n",
    "response = get_response_from_model(model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2681c3a",
   "metadata": {},
   "source": [
    "### Object Detection\n",
    "This step focuses on identifying and labeling objects within an image. The multimodal input allows GPT-4o to analyze the visual data and generate a list of objects detected in the scene. Object detection is crucial for tasks such as inventory management, autonomous driving, and augmented reality applications and such.\n",
    "\n",
    "Follow the further steps to replicate object detection using GPT-4o.\n",
    "\n",
    "To utilize the GPT-4o model, which supports both text and image inputs, use the code below. This example demonstrates how to create a prompt with an image URL and a text query, enabling the model to process and provide a response based on both visual and textual information.\n",
    "\n",
    "Note: You can replace the image URL with any image of your choice and modify the text prompt to ask the model any question about that image based on your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4c0184e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "def encode_image_from_url(image_url):\n",
    "    \"\"\"Download and encode image to base64 format from URL.\"\"\"\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        return base64.b64encode(response.content).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download image. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dd3f1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_prompt(image_url, text_prompt):\n",
    "    \"\"\"Create a prompt message for the model with the image data.\"\"\"\n",
    "    # Encode image URL to base64 format\n",
    "    image_base64 = encode_image_from_url(image_url)\n",
    "        # Create messages including both text and image input\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": text_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url  # Use the direct image URL\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c021820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_model(model_name, messages):\n",
    "    \"\"\"Send messages to the model and return the response.\"\"\"\n",
    "    kwargs = dict(model_name=model_name, messages=messages)\n",
    "    response = chat.completions.create(**kwargs)\n",
    "    return response.to_dict()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0335c5e",
   "metadata": {},
   "source": [
    "By following this example, you can easily integrate image-based inputs with the GPT-4o model and leverage its ability to understand and generate responses based on both visual and text content. For additional guidance, refer to the screenshot below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/objectDetection.jpg\" width=\"30%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ba60e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bottle is blue in color and there is one bottle in the image.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_url = \"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/objectDetection.jpg\"\n",
    "text_prompt = \"give me the bottle color and its count.\"  # Prompt asking for the description\n",
    "model_name = \"gpt-4o\"  # Replace with the model that supports image input\n",
    "\n",
    "# Create prompt with image and text\n",
    "messages = create_image_prompt(image_url, text_prompt)\n",
    "\n",
    "# Get response from model\n",
    "response = get_response_from_model(model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f5bcd",
   "metadata": {},
   "source": [
    "### Graph Analysis\n",
    "Here, the tutorial demonstrates how GPT-4o can be used to interpret and analyze data presented in graphical form. By combining text and image input, the model can extract meaningful insights from charts, graphs, and other visual data representations. This step is valuable for data analysis, reporting, and decision-making processes.\n",
    "\n",
    "Follow the further steps to replicate graph analysis using GPT-4o.\n",
    "\n",
    "To utilize the GPT-4o model, which supports both text and image inputs, use the code below. This example demonstrates how to create a prompt with an image URL and a text query, enabling the model to process and provide a response based on both visual and textual information.\n",
    "\n",
    "Note: You can replace the image URL with any image of your choice and modify the text prompt to ask the model any question about that image based on your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "65f7cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "\n",
    "def encode_image_from_url(image_url):\n",
    "    \"\"\"Download and encode image to base64 format from URL.\"\"\"\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        return base64.b64encode(response.content).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download image. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0d4866bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_prompt(image_url, text_prompt):\n",
    "    \"\"\"Create a prompt message for the model with the image data.\"\"\"\n",
    "    # Encode image URL to base64 format\n",
    "    image_base64 = encode_image_from_url(image_url)\n",
    "    \n",
    "    # Create messages including both text and image input\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": text_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url  # Use the direct image URL\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "31b4ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_model(model_name, messages):\n",
    "    \"\"\"Send messages to the model and return the response.\"\"\"\n",
    "    kwargs = dict(model_name=model_name, messages=messages)\n",
    "    response = chat.completions.create(**kwargs)\n",
    "    return response.to_dict()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020aa72",
   "metadata": {},
   "source": [
    "By following this example, you can easily integrate image-based inputs with the GPT-4o model and leverage its ability to understand and generate responses based on both visual and text content. For additional guidance, refer to the screenshot below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/graph.jpg\" width=\"30%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "966db2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This graph depicts the historical performance of the Dow Jones Industrial Average (DJIA) from around 2010 to 2024. The Dow Jones Industrial Average is a stock market index that measures the stock performance of 30 large companies listed on stock exchanges in the United States. The y-axis shows the value of the index, with notable milestones marked at 15,000, 20,000, 25,000, 30,000, 35,000, and 40,000 points. The x-axis represents the timeline spanning from 2010 to 2024. The graph illustrates the growth trend over this period, despite noticeable dips during times of market uncertainty, such as around 2020, when the COVID-19 pandemic significantly impacted financial markets.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_url = \"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/graph.jpg\"\n",
    "text_prompt = \"what is this graph about\"  # Prompt asking for the description\n",
    "model_name = \"gpt-4o\"  # Replace with the model that supports image input\n",
    "\n",
    "# Create prompt with image and text\n",
    "messages = create_image_prompt(image_url, text_prompt)\n",
    "# Get response from model\n",
    "response = get_response_from_model(model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1fa27a",
   "metadata": {},
   "source": [
    "### Math\n",
    "In this step, we explore how GPT-4o handles mathematical problems that involve both textual descriptions and visual data. The model can solve equations, interpret mathematical expressions in images, and provide detailed explanations of its reasoning. This capability is useful in educational tools, scientific research, and engineering applications.\n",
    "\n",
    "Follow the further steps to replicate mathematical operations using GPT-4o.\n",
    "\n",
    "To utilize the GPT-4o model, which supports both text and image inputs, use the code below. This example demonstrates how to create a prompt with an image URL and a text query, enabling the model to process and provide a response based on both visual and textual information.\n",
    "\n",
    "Note: You can replace the image URL with any image of your choice and modify the text prompt to ask the model any question about that image based on your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "350484f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "\n",
    "def encode_image_from_url(image_url):\n",
    "    \"\"\"Download and encode image to base64 format from URL.\"\"\"\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        return base64.b64encode(response.content).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download image. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8ad66a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_prompt(image_url, text_prompt):\n",
    "    \"\"\"Create a prompt message for the model with the image data.\"\"\"\n",
    "    # Encode image URL to base64 format\n",
    "    image_base64 = encode_image_from_url(image_url)\n",
    "    \n",
    "    # Create messages including both text and image input\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": text_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url  # Use the direct image URL\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a478ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_model(model_name, messages):\n",
    "    \"\"\"Send messages to the model and return the response.\"\"\"\n",
    "    kwargs = dict(model_name=model_name, messages=messages)\n",
    "    response = chat.completions.create(**kwargs)\n",
    "    return response.to_dict()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64512899",
   "metadata": {},
   "source": [
    "By following this example, you can easily integrate image-based inputs with the GPT-4o model and leverage its ability to understand and generate responses based on both visual and text content. For additional guidance, refer to the screenshot below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/math.jpg\" width=\"20%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5d06f96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation \\((2x - 10)/2 = 3(x - 1)\\), follow these steps:\n",
      "\n",
      "1. Distribute the division on the left side:\n",
      "\n",
      "   \\((2x - 10)/2 = x - 5\\).\n",
      "\n",
      "2. Now the equation is:\n",
      "\n",
      "   \\(x - 5 = 3(x - 1)\\).\n",
      "\n",
      "3. Distribute the 3 on the right side:\n",
      "\n",
      "   \\(x - 5 = 3x - 3\\).\n",
      "\n",
      "4. Rearrange the equation to collect like terms:\n",
      "\n",
      "   \\(x - 3x = -3 + 5\\).\n",
      "\n",
      "5. Simplify:\n",
      "\n",
      "   \\(-2x = 2\\).\n",
      "\n",
      "6. Divide both sides by \\(-2\\):\n",
      "\n",
      "   \\(x = -1\\).\n",
      "\n",
      "So, the solution is \\(x = -1\\).\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_url = \"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/math.jpg\"\n",
    "text_prompt = \"find x\"  # Prompt asking for the description\n",
    "model_name = \"gpt-4o\"  # Replace with the model that supports image input\n",
    "\n",
    "# Create prompt with image and text\n",
    "messages = create_image_prompt(image_url, text_prompt)\n",
    "\n",
    "# Get response from model\n",
    "response = get_response_from_model(model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4419e4",
   "metadata": {},
   "source": [
    "### Image to Text\n",
    "The final step focuses on converting visual information into text. By providing an image as input, GPT-4o generates a textual description or transcription of the content. This step is particularly beneficial for accessibility tools, content creation, and archiving visual data.\n",
    "\n",
    "Follow the further steps to replicate Optical Character Recognition (OCR) using GPT-4o.\n",
    "\n",
    "To utilize the GPT-4o model, which supports both text and image inputs, use the code below. This example demonstrates how to create a prompt with an image URL and a text query, enabling the model to process and provide a response based on both visual and textual information.\n",
    "\n",
    "Note: You can replace the image URL with any image of your choice and modify the text prompt to ask the model any question about that image based on your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7c90f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "def encode_image_from_url(image_url):\n",
    "    \"\"\"Download and encode image to base64 format from URL.\"\"\"\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        return base64.b64encode(response.content).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download image. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ae2cb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_prompt(image_url, text_prompt):\n",
    "    \"\"\"Create a prompt message for the model with the image data.\"\"\"\n",
    "    # Encode image URL to base64 format\n",
    "    image_base64 = encode_image_from_url(image_url)\n",
    "    \n",
    "    # Create messages including both text and image input\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": text_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url  # Use the direct image URL\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b5dc0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_model(model_name, messages):\n",
    "    \"\"\"Send messages to the model and return the response.\"\"\"\n",
    "    kwargs = dict(model_name=model_name, messages=messages)\n",
    "    response = chat.completions.create(**kwargs)\n",
    "    return response.to_dict()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41351a",
   "metadata": {},
   "source": [
    "By following this example, you can easily integrate image-based inputs with the GPT-4o model and leverage its ability to understand and generate responses based on both visual and text content. For additional guidance, refer to the screenshot below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/handwrittenText.png\" width=\"45%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "855848d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear User,\n",
      "\n",
      "Handwrytten uses robotic handwriting machines that use an actual pen to write your message. The results are virtually indistinguishable from actual handwriting.\n",
      "Try it today!\n",
      "\n",
      "The Robot\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_url = \"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/handwrittenText.png\"\n",
    "text_prompt = \"extract text\"  # Prompt asking for the description\n",
    "model_name = \"gpt-4o\"  # Replace with the model that supports image input\n",
    "\n",
    "# Create prompt with image and text\n",
    "messages = create_image_prompt(image_url, text_prompt)\n",
    "\n",
    "# Get response from model\n",
    "response = get_response_from_model(model_name, messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
