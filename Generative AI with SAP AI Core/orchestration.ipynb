{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec630573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource group is set to: default\n"
     ]
    }
   ],
   "source": [
    "from config import init_env\n",
    "from config import variables\n",
    "import importlib\n",
    "variables = importlib.reload(variables)\n",
    "init_env.set_environment_variables()\n",
    "\n",
    "# Do not modify the `assert` line below \n",
    "assert variables.RESOURCE_GROUP!='', \"\"\"You should change the value assigned to the `RESOURCE_GROUP` in the `variables.py` file to your own resource group first!\"\"\"\n",
    "print(f\"Resource group is set to: {variables.RESOURCE_GROUP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7dfa5",
   "metadata": {},
   "source": [
    "[*Tutorial01 - Consumption of GenAI models Using Orchestration - A Beginner's Guide*](https://developers.sap.com/tutorials/ai-core-orchestration-consumption.html)\n",
    "## Consumption of GenAI models Using Orchestration - A Beginner's Guide\n",
    "While orchestration in SAP AI Core offers capabilities such as data masking, content filtering, translation, and grounding, this tutorial focuses on the basic consumption flow using mandatory modules like templating and model configuration. Others modules are Optional and usage of those modules are covered in a separate tutorial.\n",
    "By the end of this tutorial,you will \n",
    "* Have a foundational understanding of orchestration through its minimal usage, focusing on practical application of templates and how to switch between different models using harmonized APIs.\n",
    "* Learn how to implement the solution using SAP AI Launchpad, Python SDK, Java, JavaScript, and Bruno.\n",
    "\n",
    "Refer to the [<u>orchestration documentation</u>](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/orchestration-8d022355037643cebf775cd3bf662cc5) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b647ad7",
   "metadata": {},
   "source": [
    "### Check Configuration for Orchestration deployment - Optional Step\n",
    "<blockquote>\n",
    "<p>In SAP AI Core, orchestration deployment is available by default in the default resource group during the onboarding. </p>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf537265",
   "metadata": {},
   "source": [
    "To check the deployment, first need to create the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b25e870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create Connection\n",
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "ai_core_client = AICoreV2Client(\n",
    "    base_url = os.environ['AICORE_BASE_URL'] + \"/v2\", # The present SAP AI Core API version is 2\n",
    "    auth_url=  os.environ['AICORE_AUTH_URL'], \n",
    "    client_id = os.environ['AICORE_CLIENT_ID'],\n",
    "    resource_group =os.environ['AICORE_RESOURCE_GROUP'],\n",
    "    client_secret = os.environ['AICORE_CLIENT_SECRET']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ebd1ea",
   "metadata": {},
   "source": [
    "Find the configuration list for orchestration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232d1e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Configuration ----------------\n",
      "id: 796aca2d-d225-4ffe-9b0b-e83e3b3f7f9f\n",
      "name: config_ss02_orchestration\n",
      "scenario_id: orchestration\n",
      "executable_id: orchestration\n",
      "parameter_bindings: []\n",
      "input_artifact_bindings: []\n",
      "created_at: 2025-09-16 02:19:08+00:00\n",
      "scenario: None\n",
      "--------------- Configuration ----------------\n",
      "id: 43b0b78c-1555-42bd-8928-4debe4f6d4fb\n",
      "name: config_ss02_orchestration\n",
      "scenario_id: orchestration\n",
      "executable_id: orchestration\n",
      "parameter_bindings: []\n",
      "input_artifact_bindings: []\n",
      "created_at: 2025-09-16 02:17:09+00:00\n",
      "scenario: None\n",
      "--------------- Configuration ----------------\n",
      "id: 01d20309-52af-42b2-a368-1ea69b79aaab\n",
      "name: config_ss02_orchestration\n",
      "scenario_id: orchestration\n",
      "executable_id: orchestration\n",
      "parameter_bindings: []\n",
      "input_artifact_bindings: []\n",
      "created_at: 2025-09-16 02:14:49+00:00\n",
      "scenario: None\n",
      "--------------- Configuration ----------------\n",
      "id: b4a884be-1a61-46fa-a3ed-3f8ebd5c1173\n",
      "name: config_ss02_orchestration\n",
      "scenario_id: orchestration\n",
      "executable_id: orchestration\n",
      "parameter_bindings: []\n",
      "input_artifact_bindings: []\n",
      "created_at: 2025-09-16 02:14:02+00:00\n",
      "scenario: None\n",
      "--------------- Configuration ----------------\n",
      "id: 98d42681-6799-4999-b0ca-b04729696a36\n",
      "name: config_ss01_orchestration\n",
      "scenario_id: orchestration\n",
      "executable_id: orchestration\n",
      "parameter_bindings: []\n",
      "input_artifact_bindings: []\n",
      "created_at: 2025-09-08 05:38:23+00:00\n",
      "scenario: None\n",
      "--------------- Configuration ----------------\n",
      "id: 30708a48-46af-4f5a-941c-361ca76713d7\n",
      "name: defaultOrchestrationConfig\n",
      "scenario_id: orchestration\n",
      "executable_id: orchestration\n",
      "parameter_bindings: []\n",
      "input_artifact_bindings: []\n",
      "created_at: 2025-08-02 04:16:21+00:00\n",
      "scenario: None\n",
      "--------------- Configuration ----------------\n",
      "id: 8cecd483-0695-4fd6-87c5-b41359fa1627\n",
      "name: orchestration\n",
      "scenario_id: orchestration\n",
      "executable_id: orchestration\n",
      "parameter_bindings: [<ai_api_client_sdk.models.parameter_binding.ParameterBinding object at 0x7fc68517f2f0>, <ai_api_client_sdk.models.parameter_binding.ParameterBinding object at 0x7fc68517f470>]\n",
      "input_artifact_bindings: []\n",
      "created_at: 2025-05-22 18:18:33+00:00\n",
      "scenario: None\n",
      "--------------- Configuration ----------------\n",
      "id: 2b2f911e-59a0-46a2-9419-3fd9db293a25\n",
      "name: defaultOrchestrationConfig\n",
      "scenario_id: orchestration\n",
      "executable_id: orchestration\n",
      "parameter_bindings: []\n",
      "input_artifact_bindings: []\n",
      "created_at: 2025-05-14 09:19:06+00:00\n",
      "scenario: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = ai_core_client.configuration.query()\n",
    "\n",
    "# Filter items with scenario_id = 'orchestration'\n",
    "orchestration_items = [vars(rg) for rg in response.resources if getattr(rg, 'scenario_id', None) == 'orchestration']\n",
    "\n",
    "# Print details of all matching items\n",
    "for item in orchestration_items:\n",
    "    print(\"--------------- Configuration ----------------\")\n",
    "    for key, value in item.items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8031956",
   "metadata": {},
   "source": [
    "Check the orchestration deployment using the onfiguration find from the previous step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb90afd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Deployment ----------------\n",
      "id: d02c3ee65a3a9f5c\n",
      "configuration_id: 796aca2d-d225-4ffe-9b0b-e83e3b3f7f9f\n",
      "configuration_name: config_ss02_orchestration\n",
      "scenario_id: orchestration\n",
      "status: Status.RUNNING\n",
      "target_status: TargetStatus.RUNNING\n",
      "created_at: 2025-09-16 02:21:08+00:00\n",
      "modified_at: 2025-11-14 07:14:55+00:00\n",
      "status_message: None\n",
      "status_details: None\n",
      "submission_time: 2025-09-16 02:23:59+00:00\n",
      "start_time: 2025-09-16 02:35:25+00:00\n",
      "completion_time: None\n",
      "deployment_url: https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d02c3ee65a3a9f5c\n",
      "last_operation: Operation.CREATE\n",
      "latest_running_configuration_id: 796aca2d-d225-4ffe-9b0b-e83e3b3f7f9f\n",
      "details: {'scaling': {'backend_details': {}}, 'resources': {'backend_details': {}}}\n",
      "ttl: None\n"
     ]
    }
   ],
   "source": [
    "response = ai_core_client.deployment.query()\n",
    "\n",
    "# Filter items with configuration_id  \n",
    "orchestration_items = [vars(rg) for rg in response.resources if getattr(rg, 'configuration_id', None) == '796aca2d-d225-4ffe-9b0b-e83e3b3f7f9f']\n",
    "\n",
    "# Print details of all matching items\n",
    "for item in orchestration_items:\n",
    "    print(\"--------------- Deployment ----------------\")\n",
    "    for key, value in item.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae24c5",
   "metadata": {},
   "source": [
    "To check the available LLM list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eec25c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: sonar-pro\n",
      "allowed_scenarios [{'executable_id': 'perplexity-ai', 'scenario_id': 'foundation-models'}]\n",
      " \n",
      "model: sonar\n",
      "allowed_scenarios [{'executable_id': 'perplexity-ai', 'scenario_id': 'foundation-models'}]\n",
      " \n",
      "model: cohere-reranker\n",
      "allowed_scenarios [{'executable_id': 'aicore-cohere', 'scenario_id': 'foundation-models'}]\n",
      " \n",
      "model: cohere--command-a-reasoning\n",
      "allowed_scenarios [{'executable_id': 'aicore-cohere', 'scenario_id': 'foundation-models'}]\n",
      " \n",
      "model: anthropic--claude-3-haiku\n",
      "allowed_scenarios [{'executable_id': 'aws-bedrock', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: anthropic--claude-3-opus\n",
      "allowed_scenarios [{'executable_id': 'aws-bedrock', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: anthropic--claude-3.5-sonnet\n",
      "allowed_scenarios [{'executable_id': 'aws-bedrock', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: anthropic--claude-3.7-sonnet\n",
      "allowed_scenarios [{'executable_id': 'aws-bedrock', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: anthropic--claude-4-sonnet\n",
      "allowed_scenarios [{'executable_id': 'aws-bedrock', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: anthropic--claude-4.5-sonnet\n",
      "allowed_scenarios [{'executable_id': 'aws-bedrock', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: amazon--titan-embed-text\n",
      "allowed_scenarios [{'executable_id': 'aws-bedrock', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: amazon--titan-embed-image\n",
      "allowed_scenarios [{'executable_id': 'aws-bedrock', 'scenario_id': 'foundation-models'}]\n",
      " \n",
      "model: amazon--nova-pro\n",
      "allowed_scenarios [{'executable_id': 'aws-bedrock', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: amazon--nova-lite\n",
      "allowed_scenarios [{'executable_id': 'aws-bedrock', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: amazon--nova-micro\n",
      "allowed_scenarios [{'executable_id': 'aws-bedrock', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: mistralai--mistral-large-instruct\n",
      "allowed_scenarios [{'executable_id': 'aicore-mistralai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: mistralai--mistral-small-instruct\n",
      "allowed_scenarios [{'executable_id': 'aicore-mistralai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: ibm--granite-13b-chat\n",
      "allowed_scenarios [{'executable_id': 'aicore-ibm', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: nvidia--llama-3.2-nv-embedqa-1b\n",
      "allowed_scenarios [{'executable_id': 'aicore-nvidia', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: meta--llama3.1-70b-instruct\n",
      "allowed_scenarios [{'executable_id': 'aicore-opensource', 'scenario_id': 'foundation-models'}]\n",
      " \n",
      "model: mistralai--mistral-medium-instruct\n",
      "allowed_scenarios [{'executable_id': 'aicore-mistralai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gemini-2.0-flash\n",
      "allowed_scenarios [{'executable_id': 'gcp-vertexai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gemini-2.0-flash-lite\n",
      "allowed_scenarios [{'executable_id': 'gcp-vertexai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gemini-2.5-pro\n",
      "allowed_scenarios [{'executable_id': 'gcp-vertexai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gemini-2.5-flash\n",
      "allowed_scenarios [{'executable_id': 'gcp-vertexai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gemini-embedding\n",
      "allowed_scenarios [{'executable_id': 'gcp-vertexai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gpt-5\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gpt-5-nano\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gpt-5-mini\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gpt-4o\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gpt-4o-mini\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gpt-4\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gpt-35-turbo\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gpt-35-turbo-0125\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: text-embedding-ada-002\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gpt-4.1\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gpt-4.1-nano\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: gpt-4.1-mini\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: o3-mini\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: o3\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: o4-mini\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: text-embedding-3-small\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n",
      "model: text-embedding-3-large\n",
      "allowed_scenarios [{'executable_id': 'azure-openai', 'scenario_id': 'foundation-models'}, {'executable_id': 'orchestration', 'scenario_id': 'orchestration'}]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "response = ai_core_client.model.query()\n",
    "for rg in response.resources:\n",
    "    print('model:', rg.model)\n",
    "    print('allowed_scenarios', rg.allowed_scenarios)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe75beb",
   "metadata": {},
   "source": [
    "### Consume LLM's in Generative AI Hub through Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0f2c5",
   "metadata": {},
   "source": [
    "Load the CV file using the following code to read its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d06c3c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Doe\n",
      "1234 Data St, San Francisco, CA 94101\n",
      "(123) 456-7890\n",
      "johndoe@email.com\n",
      "LinkedIn Profile\n",
      "GitHub Profile\n",
      "\n",
      "Objective\n",
      "Detail-oriented Data Scientist with 3+ years of experience in data analysis, statistical modeling, and machine learning. Seeking to leverage expertise in predictive modeling and data visualization to help drive data-informed decision-making at [Company Name].\n",
      "\n",
      "Education\n",
      "Master of Science in Data Science\n",
      "University of California, Berkeley\n",
      "Graduated: May 2021\n",
      "\n",
      "Bachelor of Science in Computer Science\n",
      "University of California, Los Angeles\n",
      "Graduated: May 2019\n",
      "\n",
      "Technical Skills\n",
      "\n",
      "Programming Languages: Python, R, SQL, Java\n",
      "Data Analysis & Visualization: Pandas, NumPy, Matplotlib, Seaborn, Tableau\n",
      "Machine Learning: Scikit-learn, TensorFlow, Keras, XGBoost\n",
      "Big Data Technologies: Hadoop, Spark\n",
      "Databases: MySQL, PostgreSQL\n",
      "Version Control: Git\n",
      "\n",
      "Professional Experience\n",
      "\n",
      "Data Scientist\n",
      "DataCorp Inc., San Francisco, CA\n",
      "June 2021 – Present\n",
      "\n",
      "Developed predictive models to optimize marketing campaigns, which increased ROI by 20%.\n",
      "Conducted in-depth data analysis using Python and SQL to identify trends and patterns in large datasets.\n",
      "Collaborated with cross-functional teams to implement data-driven strategies that improved customer satisfaction scores by 15%.\n",
      "Created interactive dashboards using Tableau to visualize KPIs for stakeholders.\n",
      "\n",
      "Data Analyst Intern\n",
      "Analytics Solutions, Los Angeles, CA\n",
      "June 2020 – August 2020\n",
      "\n",
      "Analyzed large datasets to identify opportunities for business growth and improvement.\n",
      "Assisted in the development of automated reporting tools using Python and Excel.\n",
      "Worked with data visualization tools to create insightful reports for management.\n",
      "\n",
      "Projects\n",
      "\n",
      "Customer Segmentation Analysis\n",
      "Conducted K-means clustering on customer data to segment the customer base into distinct groups, enabling targeted marketing strategies.\n",
      "\n",
      "Predictive Stock Price Modeling\n",
      "Built a predictive model using time series analysis to forecast stock prices, achieving an accuracy rate of 85%.\n",
      "\n",
      "Sentiment Analysis on Social Media\n",
      "Implemented natural language processing techniques to analyze sentiment from tweets, providing insights into public opinion on various topics.\n",
      "\n",
      "Certifications\n",
      "\n",
      "Certified Data Scientist (CDS) – Data Science Council of America\n",
      "Machine Learning Specialization – Coursera by Stanford University\n",
      "\n",
      "Professional Affiliations\n",
      "\n",
      "Member, Association for Computing Machinery (ACM)\n",
      "Member, Data Science Society\n",
      "\n",
      "References\n",
      "Available upon request.\n",
      "\n",
      "Personal Interests\n",
      "- I absolutely love exploring new technologies and working on innovative projects.\n",
      "- I enjoy reading books, especially on artificial intelligence and machine learning.\n",
      "- I hate people who are dishonest and unreliable.\n",
      "- I love traveling and experiencing new cultures.\n",
      "- I enjoy playing video games, especially competitive ones.\n",
      "- I hate being stuck in a routine; I always seek new challenges and growth opportunities.\n",
      "-I hate working in Azure cloud -\"Azure cloud is the most irritating platform i have ever used\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.orchestration.utils import load_text_file \n",
    "\n",
    "# Load the CV file content \n",
    "cv_file_path = \"cv.txt\" # Specify the correct path to the CV file \n",
    "cv_content = load_text_file(cv_file_path) \n",
    "\n",
    "# Print the content to verify it has been loaded \n",
    "print(cv_content) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f067fa",
   "metadata": {},
   "source": [
    "The next step involves creating a template that specifies how the AI should handle the resume content. The template will include both SystemMessage and UserMessage components.\n",
    "\n",
    "* SystemMessage: Defines the AI assistant’s role and instructions.\n",
    "\n",
    "* UserMessage: Represents the user’s input (i.e., the CV content) to be processed by the AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccfc9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.message import SystemMessage, UserMessage \n",
    "from gen_ai_hub.orchestration.models.template import Template, TemplateValue \n",
    "\n",
    "# Define the template for resume screening \n",
    "template = Template(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            \"\"\"You are a helpful AI assistant for HR.\n",
    "            Summarize the following CV in 10 sentences,focusing on key qualifications, work experience, and achievements.\n",
    "            Include personal contact information,organizational history, and personal interests\"\"\"\n",
    "        ), \n",
    "        \n",
    "        UserMessage(\n",
    "            \"Here is a candidate's resume: {{?candidate_resume}}\"\n",
    "        ),\n",
    "    ], \n",
    "\n",
    "    defaults=[\n",
    "        TemplateValue(\n",
    "            name=\"candidate_resume\", \n",
    "            value=\"John Doe's resume content goes here...\"\n",
    "        ), \n",
    "    ], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d7b729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations created successfully:\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.orchestration.models.llm import LLM \n",
    "from gen_ai_hub.orchestration.models.config import OrchestrationConfig \n",
    "\n",
    "# List of models to use，models have to be picked from the available list fetched before \n",
    "models = [\n",
    "    LLM(name=\"gpt-4o\", version=\"latest\", parameters={\"max_tokens\": 1000, \"temperature\": 0.6}), \n",
    "    LLM(name=\"mistralai--mistral-medium-instruct\", version=\"latest\", parameters={\"max_tokens\": 1000, \"temperature\": 0.6}), \n",
    "    LLM(name=\"anthropic--claude-4.5-sonnet\", version=\"latest\", parameters={\"max_tokens\": 1000, \"temperature\": 0.6}), \n",
    "    LLM(name=\"meta--llama3.1-70b-instruct\", version=\"latest\", parameters={\"max_tokens\": 1000, \"temperature\": 0.6}), \n",
    "]\n",
    "\n",
    "# Create configurations for each model \n",
    "configs = [] \n",
    "for model in models: \n",
    "    # Create orchestration config for each model \n",
    "    config = OrchestrationConfig( \n",
    "        template=template, \n",
    "        llm=model,\n",
    "    ) \n",
    "    configs.append(config) \n",
    "\n",
    "print(\"Model configurations created successfully:\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389802e1",
   "metadata": {},
   "source": [
    "Now, run the orchestration with the prepared configurations. The results will be saved in <i>model_responses.txt</i> for later review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10608845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.service import OrchestrationService \n",
    "\n",
    "# Initialize an empty list to store the responses \n",
    "responses = [] \n",
    "\n",
    "# Iterate through each config and get the response using the filtered input \n",
    "for i, config in enumerate(configs):\n",
    "    orchestration_service = OrchestrationService(\n",
    "        #api_url=ready_deployment.deployment_url, \n",
    "        config=config\n",
    "    ) \n",
    "\n",
    "    # Run orchestration with the provided input (for example, candidate resume content) \n",
    "    result = orchestration_service.run(\n",
    "        template_values=[\n",
    "            TemplateValue(\n",
    "                name=\"candidate_resume\",\n",
    "                value=cv_content # Adjust 'cv_content' as needed\n",
    "            )\n",
    "        ]\n",
    "    ) \n",
    "\n",
    "    # Extract the response content \n",
    "    response = result.orchestration_result.choices[0].message.content \n",
    "\n",
    "    # Append the response to the responses list \n",
    "    responses.append(\n",
    "        { \n",
    "            \"model\": models[i].name, # Store model name \n",
    "            \"response\": response # Store the corresponding model response \n",
    "        }\n",
    "    ) \n",
    "\n",
    "# Store the responses in a text file \n",
    "with open(\"model_responses.txt\", \"w\") as file: \n",
    "    for response_data in responses: \n",
    "        file.write(f\"Response from model {response_data['model']}:\\n\") \n",
    "        file.write(f\"{response_data['response']}\\n\") \n",
    "        file.write(\"-\" * 80 + \"\\n\") # Add a separator between model responses "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8be0e2",
   "metadata": {},
   "source": [
    "[*Tutorial02 - Leveraging Orchestration Capabilities to Enhance Responses*](https://developers.sap.com/tutorials/ai-core-orchestration-consumption-opt.html)\n",
    "## Leveraging Orchestration Capabilities to Enhance Responses\n",
    "In this tutorial, we will explore optional orchestration capabilities available in the Gen AI Hub, such as Data Masking, translation and Content Filtering.\n",
    "\n",
    "We will learn\n",
    "\n",
    "☝️ Inference of GenAI models using orchestration along with Data Masking, translation and Content Filtering features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2681c3a",
   "metadata": {},
   "source": [
    "### Accessing Orchestration Capabilities\n",
    " The focus will shift from basic orchestration to leveraging optional modules to enhance data privacy and refine response quality. These enhancements include:\n",
    "* Data Masking : Hiding sensitive information like phone numbers, organizational details, or personal identifiers. \n",
    "\n",
    "* Content Filtering : Screening for categories such as hate speech, self-harm, explicit content, and violence to ensure safe and relevant responses.\n",
    "\n",
    "* Translation : Automatically converts input and/or output text between source and target languages to support multilingual processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c0184e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Subject: Bestellung #1234567890 Verspätet - John Johnson Nachricht: Halle, ich schreibe ihnen um mich nach dem Status meiner Bestellung mit der Bestellnr. +1234567890 zu erkundigen. Die Lieferung war eigentlich für gestern geplant, ist bisher jedoch nicht erfolgt. Mein Name ist John Johnson und meine Lieferadresse lautet 125 Cole Meadows Drive Palo Alto, California 94301. Bitte lassen Sie mich per Telefon unter der Nummer +1 505802 2172 wissen, wann ich mit meiner Lieferung rechnen kann. Danke!\"\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.orchestration.utils import load_text_file \n",
    "# Load the support request file content \n",
    "support_request_path = \"support-request.txt\" # Specify the correct path to the file \n",
    "support_request = load_text_file(support_request_path) \n",
    "# Print the content to verify it has been loaded \n",
    "print(support_request)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caca596",
   "metadata": {},
   "source": [
    "### Template Configuration\n",
    "The templating module is a mandatory step in orchestration. It allows you to define dynamic inputs using placeholders, construct structured prompts, and generate a final query that will be passed to the model configuration module.\n",
    "\n",
    "In this step, we create a template that defines how the sentiment analysis prompt will be structured using message components:\n",
    "\n",
    "* system: Defines assistant behavior and task.\n",
    "\n",
    "* user: Provides the support request input.\n",
    "\n",
    "Use the following code to create the template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd3f1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.message import SystemMessage, UserMessage\n",
    "from gen_ai_hub.orchestration.models.template import Template, TemplateValue\n",
    "\n",
    "# Define the sentiment analysis template\n",
    "template = Template(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            \"\"\"You are a customer support assistant.\n",
    "             Analyze the sentiment of the user request provided and return whether the sentiment is positive, neutral, or negative. \n",
    "             Also provide a one-line justification for your classification.\"\"\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            \"Please analyze the sentiment of the following support request: {{?support_text}}\"\n",
    "        ),\n",
    "    ],\n",
    "    defaults=[\n",
    "        TemplateValue(\n",
    "            name=\"support_text\",\n",
    "            value=\"User is unhappy with the latest update and facing usability issues.\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c021820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to use，models have to be picked from the available list fetched before \n",
    "models = [\n",
    "    LLM(name=\"gpt-4o\", version=\"latest\", parameters={\"max_tokens\": 1000, \"temperature\": 0.6}), \n",
    "    LLM(name=\"mistralai--mistral-medium-instruct\", version=\"latest\", parameters={\"max_tokens\": 1000, \"temperature\": 0.6}), \n",
    "    LLM(name=\"anthropic--claude-4.5-sonnet\", version=\"latest\", parameters={\"max_tokens\": 1000, \"temperature\": 0.6}), \n",
    "    LLM(name=\"meta--llama3.1-70b-instruct\", version=\"latest\", parameters={\"max_tokens\": 1000, \"temperature\": 0.6}), \n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fc641e",
   "metadata": {},
   "source": [
    "### Setting Up Data Masking Parameters\n",
    "The **Data Masking** Module ensures data privacy by anonymizing or pseudonymizing sensitive information before it is processed.\n",
    "\n",
    "* Anonymization(匿名) : Irreversibly replaces personal identifiers with placeholders (e.g., MASKED_ENTITY). \n",
    "\n",
    "* Pseudonymization(假名) : Substitutes identifiers with reversible tokens (e.g., MASKED_ENTITY_ID).\n",
    "\n",
    "For this tutorial, we use anonymization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0335c5e",
   "metadata": {},
   "source": [
    "By following this example, you can easily integrate image-based inputs with the GPT-4o model and leverage its ability to understand and generate responses based on both visual and text content. For additional guidance, refer to the screenshot below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/objectDetection.jpg\" width=\"30%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba60e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bottle color is blue, and there is one bottle in the image.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_url = \"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/objectDetection.jpg\"\n",
    "text_prompt = \"give me the bottle color and its count.\"  # Prompt asking for the description\n",
    "model_name = \"gpt-4o\"  # Replace with the model that supports image input\n",
    "\n",
    "# Create prompt with image and text\n",
    "messages = create_image_prompt(image_url, text_prompt)\n",
    "\n",
    "# Get response from model\n",
    "response = get_response_from_model(model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f5bcd",
   "metadata": {},
   "source": [
    "### Graph Analysis\n",
    "Here, the tutorial demonstrates how GPT-4o can be used to interpret and analyze data presented in graphical form. By combining text and image input, the model can extract meaningful insights from charts, graphs, and other visual data representations. This step is valuable for data analysis, reporting, and decision-making processes.\n",
    "\n",
    "Follow the further steps to replicate graph analysis using GPT-4o.\n",
    "\n",
    "To utilize the GPT-4o model, which supports both text and image inputs, use the code below. This example demonstrates how to create a prompt with an image URL and a text query, enabling the model to process and provide a response based on both visual and textual information.\n",
    "\n",
    "Note: You can replace the image URL with any image of your choice and modify the text prompt to ask the model any question about that image based on your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65f7cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "\n",
    "def encode_image_from_url(image_url):\n",
    "    \"\"\"Download and encode image to base64 format from URL.\"\"\"\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        return base64.b64encode(response.content).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download image. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d4866bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_prompt(image_url, text_prompt):\n",
    "    \"\"\"Create a prompt message for the model with the image data.\"\"\"\n",
    "    # Encode image URL to base64 format\n",
    "    image_base64 = encode_image_from_url(image_url)\n",
    "    \n",
    "    # Create messages including both text and image input\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": text_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url  # Use the direct image URL\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31b4ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_model(model_name, messages):\n",
    "    \"\"\"Send messages to the model and return the response.\"\"\"\n",
    "    kwargs = dict(model_name=model_name, messages=messages)\n",
    "    response = chat.completions.create(**kwargs)\n",
    "    return response.to_dict()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020aa72",
   "metadata": {},
   "source": [
    "By following this example, you can easily integrate image-based inputs with the GPT-4o model and leverage its ability to understand and generate responses based on both visual and text content. For additional guidance, refer to the screenshot below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/graph.jpg\" width=\"30%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "966db2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph depicts the performance of the Dow Jones Industrial Average (DJIA) from around 2011 to 2023. The DJIA is a stock market index that represents 30 large, publicly-owned companies based in the United States. The graph shows the ups and downs of the index over this period, indicating trends, growth, and fluctuations in the market. The sharp drop around 2020 corresponds to the market impact of the COVID-19 pandemic, followed by a recovery and continued growth.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_url = \"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/graph.jpg\"\n",
    "text_prompt = \"what is this graph about\"  # Prompt asking for the description\n",
    "model_name = \"gpt-4o\"  # Replace with the model that supports image input\n",
    "\n",
    "# Create prompt with image and text\n",
    "messages = create_image_prompt(image_url, text_prompt)\n",
    "# Get response from model\n",
    "response = get_response_from_model(model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1fa27a",
   "metadata": {},
   "source": [
    "### Math\n",
    "In this step, we explore how GPT-4o handles mathematical problems that involve both textual descriptions and visual data. The model can solve equations, interpret mathematical expressions in images, and provide detailed explanations of its reasoning. This capability is useful in educational tools, scientific research, and engineering applications.\n",
    "\n",
    "Follow the further steps to replicate mathematical operations using GPT-4o.\n",
    "\n",
    "To utilize the GPT-4o model, which supports both text and image inputs, use the code below. This example demonstrates how to create a prompt with an image URL and a text query, enabling the model to process and provide a response based on both visual and textual information.\n",
    "\n",
    "Note: You can replace the image URL with any image of your choice and modify the text prompt to ask the model any question about that image based on your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "350484f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "\n",
    "def encode_image_from_url(image_url):\n",
    "    \"\"\"Download and encode image to base64 format from URL.\"\"\"\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        return base64.b64encode(response.content).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download image. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ad66a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_prompt(image_url, text_prompt):\n",
    "    \"\"\"Create a prompt message for the model with the image data.\"\"\"\n",
    "    # Encode image URL to base64 format\n",
    "    image_base64 = encode_image_from_url(image_url)\n",
    "    \n",
    "    # Create messages including both text and image input\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": text_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url  # Use the direct image URL\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a478ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_model(model_name, messages):\n",
    "    \"\"\"Send messages to the model and return the response.\"\"\"\n",
    "    kwargs = dict(model_name=model_name, messages=messages)\n",
    "    response = chat.completions.create(**kwargs)\n",
    "    return response.to_dict()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64512899",
   "metadata": {},
   "source": [
    "By following this example, you can easily integrate image-based inputs with the GPT-4o model and leverage its ability to understand and generate responses based on both visual and text content. For additional guidance, refer to the screenshot below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/math.jpg\" width=\"20%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d06f96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation \\((2x - 10)/2 = 3(x - 1)\\), follow these steps:\n",
      "\n",
      "1. Multiply both sides by 2 to eliminate the fraction:\n",
      "   \\[\n",
      "   2x - 10 = 6(x - 1)\n",
      "   \\]\n",
      "\n",
      "2. Distribute the 6 on the right-hand side:\n",
      "   \\[\n",
      "   2x - 10 = 6x - 6\n",
      "   \\]\n",
      "\n",
      "3. Rearrange the equation to bring all terms involving \\(x\\) on one side and constant terms on the other side. Subtract \\(2x\\) from both sides:\n",
      "   \\[\n",
      "   -10 = 4x - 6\n",
      "   \\]\n",
      "\n",
      "4. Add 6 to both sides to isolate the term with \\(x\\):\n",
      "   \\[\n",
      "   -4 = 4x\n",
      "   \\]\n",
      "\n",
      "5. Divide both sides by 4 to solve for \\(x\\):\n",
      "   \\[\n",
      "   x = -1\n",
      "   \\]\n",
      "\n",
      "Thus, the solution is \\(x = -1\\).\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_url = \"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/math.jpg\"\n",
    "text_prompt = \"find x\"  # Prompt asking for the description\n",
    "model_name = \"gpt-4o\"  # Replace with the model that supports image input\n",
    "\n",
    "# Create prompt with image and text\n",
    "messages = create_image_prompt(image_url, text_prompt)\n",
    "\n",
    "# Get response from model\n",
    "response = get_response_from_model(model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4419e4",
   "metadata": {},
   "source": [
    "### Image to Text\n",
    "The final step focuses on converting visual information into text. By providing an image as input, GPT-4o generates a textual description or transcription of the content. This step is particularly beneficial for accessibility tools, content creation, and archiving visual data.\n",
    "\n",
    "Follow the further steps to replicate Optical Character Recognition (OCR) using GPT-4o.\n",
    "\n",
    "To utilize the GPT-4o model, which supports both text and image inputs, use the code below. This example demonstrates how to create a prompt with an image URL and a text query, enabling the model to process and provide a response based on both visual and textual information.\n",
    "\n",
    "Note: You can replace the image URL with any image of your choice and modify the text prompt to ask the model any question about that image based on your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "def encode_image_from_url(image_url):\n",
    "    \"\"\"Download and encode image to base64 format from URL.\"\"\"\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        return base64.b64encode(response.content).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download image. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae2cb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_prompt(image_url, text_prompt):\n",
    "    \"\"\"Create a prompt message for the model with the image data.\"\"\"\n",
    "    # Encode image URL to base64 format\n",
    "    image_base64 = encode_image_from_url(image_url)\n",
    "    \n",
    "    # Create messages including both text and image input\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": text_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url  # Use the direct image URL\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5dc0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_model(model_name, messages):\n",
    "    \"\"\"Send messages to the model and return the response.\"\"\"\n",
    "    kwargs = dict(model_name=model_name, messages=messages)\n",
    "    response = chat.completions.create(**kwargs)\n",
    "    return response.to_dict()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41351a",
   "metadata": {},
   "source": [
    "By following this example, you can easily integrate image-based inputs with the GPT-4o model and leverage its ability to understand and generate responses based on both visual and text content. For additional guidance, refer to the screenshot below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/handwrittenText.png\" width=\"45%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "855848d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear User,\n",
      "\n",
      "Handwrytten uses robotic handwriting\n",
      "machines that use an actual pen to\n",
      "write your message. The results are\n",
      "virtually indistinguishable from actual\n",
      "handwriting.\n",
      "Try it today!\n",
      "\n",
      "The Robot\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image_url = \"https://raw.githubusercontent.com/SAP-samples/ai-core-samples/main/09_BusinessAIWeek/images/handwrittenText.png\"\n",
    "text_prompt = \"extract text\"  # Prompt asking for the description\n",
    "model_name = \"gpt-4o\"  # Replace with the model that supports image input\n",
    "\n",
    "# Create prompt with image and text\n",
    "messages = create_image_prompt(image_url, text_prompt)\n",
    "\n",
    "# Get response from model\n",
    "response = get_response_from_model(model_name, messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
